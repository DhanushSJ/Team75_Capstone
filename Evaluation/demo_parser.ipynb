{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q PyMuPDF pillow imagehash nltk torch torchvision ftfy regex tqdm\n",
    "%pip install -q git+https://github.com/openai/CLIP.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "import shutil\n",
    "import io\n",
    "\n",
    "# Download NLTK data\n",
    "# NLTK 3.9+ requires 'punkt_tab' instead of 'punkt'\n",
    "try:\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "except:\n",
    "    # Fallback to old 'punkt' for older NLTK versions\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "# NLTK 3.9+ requires 'averaged_perceptron_tagger_eng' instead of 'averaged_perceptron_tagger'\n",
    "try:\n",
    "    nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "except:\n",
    "    # Fallback to old 'averaged_perceptron_tagger' for older NLTK versions\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  PDF: Team 75 Report.pdf\n",
      "  Output: analysis_output\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "PDF_PATH = \"Team 75 Report.pdf\"\n",
    "#PDF_PATH = \"Team_99_report.pdf\"\n",
    "OUTPUT_FOLDER = \"analysis_output\"\n",
    "CLEANED_TEXT_FILE = os.path.join(OUTPUT_FOLDER, \"cleaned_text.txt\")\n",
    "IMAGES_FOLDER = os.path.join(OUTPUT_FOLDER, \"images\")\n",
    "REPORT_FILE = os.path.join(OUTPUT_FOLDER, \"analysis_report.txt\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  PDF: {PDF_PATH}\")\n",
    "print(f\"  Output: {OUTPUT_FOLDER}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing output directories...\n",
      "  ✓ Cleaned old output directory\n",
      "  ✓ Created fresh output directories\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean output directory for fresh start\n",
    "def clean_output_directory(output_folder, images_folder):\n",
    "    \"\"\"Remove old output and create fresh directories\"\"\"\n",
    "    if os.path.exists(output_folder):\n",
    "        try:\n",
    "            shutil.rmtree(output_folder)\n",
    "            print(f\"  ✓ Cleaned old output directory\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠ Warning: Could not clean directory: {e}\")\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    os.makedirs(images_folder, exist_ok=True)\n",
    "    print(f\"  ✓ Created fresh output directories\")\n",
    "\n",
    "print(\"Preparing output directories...\")\n",
    "clean_output_directory(OUTPUT_FOLDER, IMAGES_FOLDER)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Extraction & Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to: analysis_output\\cleaned_text.txt\n"
     ]
    }
   ],
   "source": [
    "def normalize(s):\n",
    "    \"\"\"Normalize string for matching\"\"\"\n",
    "    return ' '.join(s.strip().split()).lower()\n",
    "\n",
    "def detect_repeating_patterns(pdf_path, min_occurrences=3):\n",
    "    \"\"\"Detect repeating headers/footers\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    header_candidates = {}\n",
    "    footer_candidates = {}\n",
    "    \n",
    "    for page in doc:\n",
    "        rect = page.rect\n",
    "        \n",
    "        # Header area (top 100px)\n",
    "        header_rect = fitz.Rect(rect.x0, rect.y0, rect.x1, rect.y0 + 100)\n",
    "        header_lines = [l.strip() for l in page.get_text(\"text\", clip=header_rect).split('\\n') \n",
    "                       if l.strip() and len(l.strip()) > 2 and not l.strip().isdigit()]\n",
    "        \n",
    "        # Footer area (bottom 100px)\n",
    "        footer_rect = fitz.Rect(rect.x0, rect.y1 - 100, rect.x1, rect.y1)\n",
    "        footer_lines = [l.strip() for l in page.get_text(\"text\", clip=footer_rect).split('\\n') \n",
    "                       if l.strip() and len(l.strip()) > 2 and not l.strip().isdigit()]\n",
    "        \n",
    "        for line in header_lines:\n",
    "            header_candidates[line] = header_candidates.get(line, 0) + 1\n",
    "        for line in footer_lines:\n",
    "            footer_candidates[line] = footer_candidates.get(line, 0) + 1\n",
    "    \n",
    "    headers = {line for line, count in header_candidates.items() if count >= min_occurrences}\n",
    "    footers = {line for line, count in footer_candidates.items() if count >= min_occurrences}\n",
    "    \n",
    "    return headers, footers\n",
    "\n",
    "def extract_text_without_headers_footers(pdf_path):\n",
    "    \"\"\"Extract text with header/footer removal\"\"\"\n",
    "    \n",
    "    # Detect patterns\n",
    "    headers, footers = detect_repeating_patterns(pdf_path)\n",
    "    headers_norm = {normalize(h) for h in headers}\n",
    "    footers_norm = {normalize(f) for f in footers}\n",
    "    \n",
    "    # Extract and filter\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_lines = []\n",
    "    removed_count = 0\n",
    "    \n",
    "    for page in doc:\n",
    "        page_text = page.get_text(\"text\")\n",
    "        for line in page_text.split('\\n'):\n",
    "            line_norm = normalize(line)\n",
    "            \n",
    "            # Skip empty\n",
    "            if not line_norm:\n",
    "                continue\n",
    "            \n",
    "            # Skip headers/footers\n",
    "            if line_norm in headers_norm or line_norm in footers_norm:\n",
    "                removed_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Skip page numbers\n",
    "            if line.strip().isdigit():\n",
    "                continue\n",
    "            \n",
    "            # Skip separator lines\n",
    "            if re.match(r'^[_\\-\\s]+$', line):\n",
    "                continue\n",
    "            \n",
    "            all_lines.append(line)\n",
    "    \n",
    "    full_text = '\\n'.join(all_lines)\n",
    "    \n",
    "    return full_text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean extracted text - remove TOC, references, appendix, etc.\"\"\"\n",
    "    \n",
    "    # 1. Extract from ABSTRACT onwards\n",
    "    abstract_start_match = re.search(r'\\s*ABSTRACT', text, re.IGNORECASE)\n",
    "    if abstract_start_match:\n",
    "        text_from_abstract_onwards = text[abstract_start_match.start():]\n",
    "    else:\n",
    "        print(\"  ⚠ Warning: ABSTRACT section not found.\")\n",
    "        text_from_abstract_onwards = text\n",
    "\n",
    "    # 2. Remove REFERENCES/BIBLIOGRAPHY and everything after\n",
    "    references_match = re.search(r'(^\\s*(References|REFERENCES):?\\s*$)|(^\\s*\\[\\d+\\].*)', text_from_abstract_onwards, re.MULTILINE)\n",
    "    if references_match:\n",
    "        text_before_references = text_from_abstract_onwards[:references_match.start()].strip()\n",
    "    else:\n",
    "        print(\"  ⚠ Warning: References section not found.\")\n",
    "        text_before_references = text_from_abstract_onwards\n",
    "\n",
    "    # 3. Remove TOC, LIST OF FIGURES, LIST OF TABLES\n",
    "    list_patterns = [\n",
    "        r'^\\s*TABLE OF CONTENTS.*?^\\s*(INTRODUCTION|Chapter\\s+I[:\\s\\.]|1\\.\\s+Introduction)', \n",
    "        r'^\\s*LIST OF FIGURES.*?^\\s*(INTRODUCTION|Chapter\\s+I[:\\s\\.]|1\\.\\s+Introduction)', \n",
    "        r'^\\s*LIST OF TABLES.*?^\\s*(INTRODUCTION|Chapter\\s+I[:\\s\\.]|1\\.\\s+Introduction)'\n",
    "    ]\n",
    "    \n",
    "    cleaned_text = text_before_references\n",
    "    found_introduction = False\n",
    "\n",
    "    for pattern_str in list_patterns:\n",
    "        pattern = re.compile(pattern_str, re.IGNORECASE | re.DOTALL | re.MULTILINE)\n",
    "        match = pattern.search(cleaned_text)\n",
    "        if match:\n",
    "            stop_marker = match.group(1)\n",
    "            if stop_marker:\n",
    "                cleaned_text = pattern.sub(stop_marker, cleaned_text, count=1).strip()\n",
    "                if re.match(r'INTRODUCTION|Chapter|1\\.', stop_marker, re.IGNORECASE):\n",
    "                    found_introduction = True\n",
    "            else:\n",
    "                cleaned_text = pattern.sub('', cleaned_text, count=1).strip()\n",
    "\n",
    "    # Final trim\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "full_text = extract_text_without_headers_footers(PDF_PATH)\n",
    "# Strategy: Skip past List of Figures/Tables content and find first content section\n",
    "# Strategy: Find real content AFTER List of Figures table ends\n",
    "# The LOF heading is followed by the table content (Figure No., Title, Page No., entries)\n",
    "# Real content (Introduction, etc.) comes after this table\n",
    "\n",
    "lof_match = re.search(r'^\\s*(List of Figures|List of Tables)\\s*$', full_text, re.IGNORECASE | re.MULTILINE)\n",
    "\n",
    "if lof_match:\n",
    "    \n",
    "    # Skip past the LOF heading AND its table content\n",
    "    # Look for real content starting at least 150 chars after LOF heading\n",
    "    search_start = lof_match.end() + 150  # Skip past table content\n",
    "    search_text = full_text[search_start:]\n",
    "    \n",
    "    # Look for first real section heading\n",
    "    content_patterns = [\n",
    "        r'(^\\s*Introduction\\s*$)',\n",
    "        r'(^\\s*INTRODUCTION\\s*$)',\n",
    "        r'(^\\s*Problem Statement\\s*$)',\n",
    "        r'(^\\s*PROBLEM STATEMENT\\s*$)'\n",
    "    ]\n",
    "    \n",
    "    content_start = None\n",
    "    for pattern in content_patterns:\n",
    "        match = re.search(pattern, search_text, re.MULTILINE)\n",
    "        if match:\n",
    "            # Verify it's real content (has paragraph text after it)\n",
    "            text_after = search_text[match.end():match.end()+200]\n",
    "            has_content = any(len(line.strip()) > 50 for line in text_after.split('\\n'))\n",
    "            \n",
    "            if has_content:\n",
    "                content_start = search_start + match.start()\n",
    "                break\n",
    "    \n",
    "    if content_start:\n",
    "        full_text = full_text[content_start:]\n",
    "    else:\n",
    "        # Fallback: just skip past LOF + 800 chars\n",
    "        full_text = full_text[lof_match.end() + 300:]\n",
    "\n",
    "# Remove references - look for References heading\n",
    "ref_heading_match = re.search(r'^\\s*(References|REFERENCES|References and Bibliography|REFERENCES/BIBLIOGRAPHY|Bibliography|BIBLIOGRAPHY):?\\s*$', full_text, re.MULTILINE)\n",
    "\n",
    "if ref_heading_match:\n",
    "    full_text = full_text[:ref_heading_match.start()]\n",
    "\n",
    "\n",
    "\n",
    "# Add blank line before headings for clarity\n",
    "def add_spacing_before_headings(text):\n",
    "    \"\"\"Add a blank line before section headings for better readability\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    formatted_lines = []\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        # Check if this line is a heading\n",
    "        is_heading = False\n",
    "        \n",
    "        if stripped and len(stripped) < 100:\n",
    "            # Pattern 1: All caps headings (e.g., \"INTRODUCTION\", \"ABSTRACT AND SCOPE\")\n",
    "            if stripped.isupper() and len(stripped.split()) <= 8:\n",
    "                is_heading = True\n",
    "            \n",
    "            # Pattern 2: Numbered sections (e.g., \"6.1.1\", \"6.1 :\", \"1.\")\n",
    "            elif re.match(r'^\\d+\\.\\d*\\.?\\d*\\s*:?\\s*$', stripped):\n",
    "                is_heading = True\n",
    "            \n",
    "            # Pattern 3: Numbered sections with text (e.g., \"6.1.1 Introduction:\")\n",
    "            elif re.match(r'^\\d+\\.\\d*\\.?\\d*\\s*[:\\-]?\\s*[A-Z]', stripped):\n",
    "                is_heading = True\n",
    "            \n",
    "            # Pattern 4: Reference markers (e.g., \"Reference [1]\")\n",
    "            elif re.match(r'^Reference\\s*\\[\\d+\\]', stripped, re.IGNORECASE):\n",
    "                is_heading = True\n",
    "            \n",
    "            # Pattern 5: Title case headings at start of line (e.g., \"Introduction\", \"Problem Statement\")\n",
    "            elif re.match(r'^[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\s*$', stripped) and len(stripped.split()) <= 5:\n",
    "                is_heading = True\n",
    "            \n",
    "            # Pattern 6: Single word capitalized (e.g., \"Objectives\", \"Evaluation\")\n",
    "            elif re.match(r'^[A-Z][a-z]+:?\\s*$', stripped) and len(stripped) > 3:\n",
    "                # Make sure previous line is not part of a sentence\n",
    "                if i > 0:\n",
    "                    prev_line = lines[i-1].strip()\n",
    "                    # If previous line ends with period, this could be a heading\n",
    "                    if not prev_line or prev_line.endswith('.') or prev_line.endswith(':'):\n",
    "                        is_heading = True\n",
    "        \n",
    "        # Add blank line before heading (if not first line and previous line isn't blank)\n",
    "        if is_heading and i > 0:\n",
    "            prev_line = lines[i - 1].strip() if i > 0 else ''\n",
    "            if prev_line:  # Only add if previous line isn't already blank\n",
    "                formatted_lines.append('')\n",
    "        \n",
    "        formatted_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(formatted_lines)\n",
    "\n",
    "cleaned_text = full_text.strip()\n",
    "cleaned_text = add_spacing_before_headings(cleaned_text)\n",
    "\n",
    "with open(CLEANED_TEXT_FILE, 'w', encoding='utf-8') as f:\n",
    "    f.write(cleaned_text)\n",
    "print(f\"✓ Saved to: {CLEANED_TEXT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heading Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Found: 15/16 expected sections\n",
      "  Missing: LIST OF TABLES\n"
     ]
    }
   ],
   "source": [
    "def extract_headings_by_fontsize(pdf_path, min_fontsize=14):\n",
    "    \"\"\"Extract text with font size >= min_fontsize\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    headings = set()\n",
    "    \n",
    "    for page in doc:\n",
    "        page_dict = page.get_text(\"dict\")\n",
    "        for block in page_dict[\"blocks\"]:\n",
    "            if block[\"type\"] == 0:  # Text block\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        if span[\"size\"] >= min_fontsize:\n",
    "                            text = span[\"text\"].strip()\n",
    "                            if text:\n",
    "                                headings.add(text.lower())\n",
    "    \n",
    "    return headings\n",
    "\n",
    "def validate_headings(pdf_path, min_fontsize=14):\n",
    "    \"\"\"Check if expected sections are present in the PDF\"\"\"\n",
    "    \n",
    "    # Expected sections based on table of contents\n",
    "    EXPECTED_SECTIONS = [\n",
    "        [\"DECLARATION\", \"Declaration\"],\n",
    "        [\"ACKNOWLEDGEMENT\", \"Acknowledgement\"],\n",
    "        [\"TABLE OF CONTENTS\", \"Table of Contents\"],\n",
    "        [\"LIST OF FIGURES\", \"List of Figures\"],\n",
    "        [\"LIST OF TABLES\", \"List of Tables\"],\n",
    "        [\"INTRODUCTION\"],\n",
    "        [\"PROBLEM STATEMENT\"],\n",
    "        [\"ABSTRACT AND SCOPE\"],\n",
    "        [\"RESEARCH / TECHNOLOGY GAP AND CHALLENGES\", \"RESEARCH\", \"TECHNOLOGY GAP\"],\n",
    "        [\"OBJECTIVES\"],\n",
    "        [\"LITERATURE SURVEY\"],\n",
    "        [\"Overview of Datasets\"],\n",
    "        [\"CONCLUSION OF CAPSTONE PROJECT PHASE - 1\", \"CONCLUSION\"],\n",
    "        [\"PLAN OF WORK FOR CAPSTONE PROJECT PHASE - 2\", \"PLAN OF WORK\"],\n",
    "        [\"REFERENCES/BIBLIOGRAPHY\", \"REFERENCES\", \"BIBLIOGRAPHY\"],\n",
    "        [\"APPENDIX A DEFINITIONS, ACRONYMS, AND ABBREVIATIONS\", \"APPENDIX\"]\n",
    "    ]\n",
    "    \n",
    "    extracted_headings = extract_headings_by_fontsize(pdf_path, min_fontsize)\n",
    "    \n",
    "    found = []\n",
    "    missing = []\n",
    "    \n",
    "    for group in EXPECTED_SECTIONS:\n",
    "        # Check if any synonym matches (handle compound headings with AND or /)\n",
    "        is_found = False\n",
    "        \n",
    "        for syn in group:\n",
    "            syn_lower = syn.lower()\n",
    "            \n",
    "            # Direct substring match\n",
    "            if any(syn_lower in heading for heading in extracted_headings):\n",
    "                is_found = True\n",
    "                break\n",
    "            \n",
    "            # Handle compound headings: \"ABSTRACT AND SCOPE\" or \"RESEARCH / TECHNOLOGY GAP\"\n",
    "            # Split by AND or / and check if ANY component matches\n",
    "            if ' and ' in syn_lower or '/' in syn:\n",
    "                components = re.split(r'\\s+and\\s+|/', syn_lower)\n",
    "                components = [c.strip() for c in components if c.strip()]\n",
    "                \n",
    "                # Check if any component is found\n",
    "                if any(comp in heading for comp in components for heading in extracted_headings):\n",
    "                    is_found = True\n",
    "                    break\n",
    "        \n",
    "        if is_found:\n",
    "            found.append(group[0])\n",
    "        else:\n",
    "            missing.append(group[0])\n",
    "    \n",
    "    return found, missing, len(EXPECTED_SECTIONS)\n",
    "\n",
    "found, missing, total = validate_headings(PDF_PATH)\n",
    "print(f\"\\n✓ Found: {len(found)}/{total} expected sections\")\n",
    "if missing:\n",
    "    print(f\"  Missing: {', '.join(missing)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Extraction & CLIP Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3: IMAGE EXTRACTION\n",
      "============================================================\n",
      "\n",
      "Extracting images and nearby text...\n",
      "  Content starts at page 5\n",
      "\n",
      "✓ Extracted 9 unique images\n"
     ]
    }
   ],
   "source": [
    "def find_content_start_page(pdf_path):\n",
    "    \"\"\"\n",
    "    Find the page where main content starts (after List of Figures/Tables).\n",
    "    Looks for \"INTRODUCTION\" or \"ABSTRACT\" as starting point.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text(\"text\").upper()\n",
    "        \n",
    "        # Look for introduction or abstract section\n",
    "        if re.search(r'\\b(INTRODUCTION|ABSTRACT AND SCOPE|CHAPTER\\s+I)\\b', text):\n",
    "            print(f\"  Content starts at page {page_num + 1}\")\n",
    "            return page_num\n",
    "        \n",
    "        # Also check if we're past \"LIST OF FIGURES\"\n",
    "        if \"LIST OF FIGURES\" in text:\n",
    "            # Content likely starts on next page or soon after\n",
    "            print(f\"  Found 'List of Figures' on page {page_num + 1}, starting extraction from page {page_num + 2}\")\n",
    "            return page_num + 1\n",
    "    \n",
    "    # If not found, start from page 10 (conservative default)\n",
    "    print(f\"  Could not find content start, defaulting to page 10\")\n",
    "    return 9\n",
    "\n",
    "def extract_images_with_text(pdf_path, output_folder, margin=100):\n",
    "    \"\"\"Extract unique images and nearby text (only from main content pages)\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    seen_hashes = set()\n",
    "    image_data = []\n",
    "    \n",
    "    # Find where main content starts\n",
    "    start_page = find_content_start_page(pdf_path)\n",
    "    \n",
    "    for page_num, page in enumerate(doc):\n",
    "        # Skip pages before content starts\n",
    "        if page_num < start_page:\n",
    "            continue\n",
    "        \n",
    "        images = page.get_images(full=True)\n",
    "        page_dict = page.get_text(\"dict\")\n",
    "        \n",
    "        for img_index, img in enumerate(images):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            \n",
    "            # Check if unique\n",
    "            pil_image = Image.open(io.BytesIO(image_bytes))\n",
    "            normalized = pil_image.convert(\"RGB\").resize((256, 256))\n",
    "            img_hash = imagehash.phash(normalized)\n",
    "            \n",
    "            if img_hash in seen_hashes:\n",
    "                continue\n",
    "            seen_hashes.add(img_hash)\n",
    "            \n",
    "            # Save image\n",
    "            filename = f\"image_p{page_num+1}_i{img_index+1}.{image_ext}\"\n",
    "            filepath = os.path.join(output_folder, filename)\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                f.write(image_bytes)\n",
    "            \n",
    "            # Extract nearby text (with header/footer filtering)\n",
    "            img_rect = None\n",
    "            for block in page_dict[\"blocks\"]:\n",
    "                if block[\"type\"] == 1:  # Image block\n",
    "                    img_blocks = [b for b in page_dict[\"blocks\"] if b[\"type\"] == 1]\n",
    "                    if img_index < len(img_blocks):\n",
    "                        img_rect = fitz.Rect(img_blocks[img_index][\"bbox\"])\n",
    "                        break\n",
    "            \n",
    "            nearby_text = \"\"\n",
    "            if img_rect:\n",
    "                # Get text blocks (paragraphs) from area around image\n",
    "                # Use larger area to ensure we get complete paragraphs\n",
    "                expanded_rect = fitz.Rect(\n",
    "                    max(0, img_rect.x0 - 200),\n",
    "                    max(0, img_rect.y0 - 200),\n",
    "                    min(page.rect.width, img_rect.x1 + 200),\n",
    "                    min(page.rect.height, img_rect.y1 + 200)\n",
    "                )\n",
    "                \n",
    "                # Get text with layout preservation\n",
    "                raw_text = page.get_text(\"text\", clip=expanded_rect)\n",
    "                \n",
    "                # Split into paragraphs (separated by double newlines or single newline with short lines)\n",
    "                paragraphs = []\n",
    "                current_para = []\n",
    "                \n",
    "                for line in raw_text.split('\\n'):\n",
    "                    line_stripped = line.strip()\n",
    "                    \n",
    "                    # Skip obvious headers/footers\n",
    "                    if line_stripped.lower() in ['dept. of cse', 'department of cse']:\n",
    "                        continue\n",
    "                    if re.match(r'^(aug|jan|feb|mar|apr|may|jun|jul|sep|oct|nov|dec)[-\\s]+(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec),?\\s*\\d{4}$', line_stripped, re.IGNORECASE):\n",
    "                        continue\n",
    "                    if line_stripped.lower() == 'capstone tracker with an integrated evaluation system':\n",
    "                        continue\n",
    "                    if re.match(r'^\\d+$', line_stripped) and len(line_stripped) <= 3:\n",
    "                        continue\n",
    "                    if re.match(r'^[_\\-]+$', line_stripped):\n",
    "                        continue\n",
    "                    \n",
    "                    if not line_stripped:\n",
    "                        # Empty line - end current paragraph\n",
    "                        if current_para:\n",
    "                            paragraphs.append(' '.join(current_para))\n",
    "                            current_para = []\n",
    "                    else:\n",
    "                        current_para.append(line_stripped)\n",
    "                \n",
    "                # Don't forget the last paragraph\n",
    "                if current_para:\n",
    "                    paragraphs.append(' '.join(current_para))\n",
    "                \n",
    "                # Filter and select COMPLETE paragraphs (no cut-off sentences)\n",
    "                good_paragraphs = []\n",
    "                for para in paragraphs:\n",
    "                    # Skip very short paragraphs (likely fragments)\n",
    "                    if len(para) < 30:\n",
    "                        continue\n",
    "                    \n",
    "                    # IMPORTANT: Skip paragraphs that start with lowercase (cut-off mid-sentence)\n",
    "                    if para and para[0].islower():\n",
    "                        continue\n",
    "                    \n",
    "                    # IMPORTANT: Skip paragraphs that don't end properly (no punctuation)\n",
    "                    if para and para[-1] not in '.!?':\n",
    "                        # Unless it's a figure caption or title\n",
    "                        if not re.search(r'\\b(Figure|Fig\\.|Table|Chapter|Section)\\s*\\d+', para, re.IGNORECASE):\n",
    "                            continue\n",
    "                    \n",
    "                    # Prefer paragraphs with figure captions\n",
    "                    if re.search(r'\\b(Figure|Fig\\.|Table)\\s*\\d+', para, re.IGNORECASE):\n",
    "                        good_paragraphs.insert(0, para)  # Put at front\n",
    "                    else:\n",
    "                        good_paragraphs.append(para)\n",
    "                \n",
    "                # Take up to 2-3 complete paragraphs (better context for CLIP)\n",
    "                selected_paras = []\n",
    "                total_length = 0\n",
    "                max_chars = 1000  # Allow more chars for complete thoughts\n",
    "                \n",
    "                for para in good_paragraphs[:5]:  # Check first 5 paragraphs\n",
    "                    if total_length + len(para) < max_chars:\n",
    "                        selected_paras.append(para)\n",
    "                        total_length += len(para)\n",
    "                    elif total_length == 0:  # If first paragraph is long, take it anyway\n",
    "                        selected_paras.append(para[:max_chars])\n",
    "                        break\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                # If we have at least one paragraph, use it\n",
    "                if selected_paras:\n",
    "                    nearby_text = '\\n\\n'.join(selected_paras)\n",
    "                else:\n",
    "                    # Fallback: look for any complete sentence\n",
    "                    all_text = ' '.join(paragraphs)\n",
    "                    # Find first capital letter start\n",
    "                    sentences = re.split(r'(?<=[.!?])\\s+', all_text)\n",
    "                    complete_sentences = [s for s in sentences if s and s[0].isupper() and len(s) > 20]\n",
    "                    if complete_sentences:\n",
    "                        nearby_text = ' '.join(complete_sentences[:3])\n",
    "                    else:\n",
    "                        nearby_text = all_text[:500] if all_text else \"\"\n",
    "            \n",
    "            # Save nearby text to .txt file (for demos)\n",
    "            text_filename = f\"image_p{page_num+1}_i{img_index+1}_text.txt\"\n",
    "            text_filepath = os.path.join(output_folder, text_filename)\n",
    "            with open(text_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(nearby_text.strip())\n",
    "            \n",
    "            # Store in memory for CLIP processing\n",
    "            image_data.append({\n",
    "                'path': filepath,\n",
    "                'text': nearby_text.strip(),\n",
    "                'page': page_num + 1,\n",
    "                'text_file': text_filepath  # Also store path to text file\n",
    "            })\n",
    "    \n",
    "    return image_data\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 3: IMAGE EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nExtracting images and nearby text...\")\n",
    "\n",
    "image_data = extract_images_with_text(PDF_PATH, IMAGES_FOLDER)\n",
    "print(f\"\\n✓ Extracted {len(image_data)} unique images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading CLIP model...\n",
      "✓ CLIP model loaded (device: cpu)\n",
      "\n",
      "Calculating image-text similarities...\n",
      "  Processed 5/9 images\n",
      "Proccessed all pictures\n",
      "\n",
      "✓ Average similarity: 0.2028\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP model\n",
    "print(\"\\nLoading CLIP model...\")\n",
    "try:\n",
    "    import clip\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    print(f\"✓ CLIP model loaded (device: {device})\")\n",
    "    clip_available = True\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not load CLIP: {e}\")\n",
    "    clip_available = False\n",
    "\n",
    "def calculate_clip_similarity(image_path, text):\n",
    "    \"\"\"Calculate cosine similarity between image and text using CLIP\"\"\"\n",
    "    if not clip_available or not text.strip():\n",
    "        return 0.0\n",
    "    \n",
    "    try:\n",
    "        # Encode image\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "        image_features = model.encode_image(image)\n",
    "        \n",
    "        # Encode text (truncate if too long)\n",
    "        text_truncated = text[:200]\n",
    "        text_tokens = clip.tokenize([text_truncated], truncate=True).to(device)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (image_features @ text_features.T).item()\n",
    "        \n",
    "        return similarity\n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {image_path}: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "# Calculate similarities\n",
    "if clip_available and image_data:\n",
    "    print(\"\\nCalculating image-text similarities...\")\n",
    "    similarities = []\n",
    "    for i, img_info in enumerate(image_data, 1):\n",
    "        sim = calculate_clip_similarity(img_info['path'], img_info['text'])\n",
    "        similarities.append(sim)\n",
    "        img_info['similarity'] = sim\n",
    "        if i % 5 == 0:\n",
    "            print(f\"  Processed {i}/{len(image_data)} images\")\n",
    "    print(\"Proccessed all pictures\")\n",
    "    avg_similarity = sum(similarities) / len(similarities) if similarities else 0\n",
    "    print(f\"\\n✓ Average similarity: {avg_similarity:.4f}\")\n",
    "else:\n",
    "    print(\"\\n⚠ Skipping CLIP analysis\")\n",
    "    similarities = []\n",
    "    avg_similarity = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Gunning Fog Index: 17.50\n",
      "  Automated Readability Index: 14.37\n",
      "  Lexical Density: 0.6403\n",
      "  Indecisive Word Index: 0\n"
     ]
    }
   ],
   "source": [
    "# Quality metric functions\n",
    "def count_syllables(word):\n",
    "    \"\"\"Count syllables in a word\"\"\"\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    previous_was_vowel = False\n",
    "    \n",
    "    for char in word:\n",
    "        is_vowel = char in vowels\n",
    "        if is_vowel and not previous_was_vowel:\n",
    "            count += 1\n",
    "        previous_was_vowel = is_vowel\n",
    "    \n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count = 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "def gunning_fog_index(text):\n",
    "    \"\"\"Calculate Gunning Fog Index\"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    \n",
    "    if not sentences or not words:\n",
    "        return 0\n",
    "    \n",
    "    complex_words = [w for w in words if count_syllables(w) >= 3]\n",
    "    \n",
    "    avg_sentence_length = len(words) / len(sentences)\n",
    "    percent_complex = (len(complex_words) / len(words)) * 100\n",
    "    \n",
    "    return 0.4 * (avg_sentence_length + percent_complex)\n",
    "\n",
    "def automated_readability_index(text):\n",
    "    \"\"\"Calculate Automated Readability Index\"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    \n",
    "    if not sentences or not words:\n",
    "        return 0\n",
    "    \n",
    "    chars = sum(len(w) for w in words)\n",
    "    \n",
    "    return 4.71 * (chars / len(words)) + 0.5 * (len(words) / len(sentences)) - 21.43\n",
    "\n",
    "def calculate_lexical_density(text):\n",
    "    \"\"\"Calculate lexical density (content words / total words)\"\"\"\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    \n",
    "    if not words:\n",
    "        return 0\n",
    "    \n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    content_tags = {'NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS'}\n",
    "    content_words = [w for w, tag in pos_tags if tag in content_tags]\n",
    "    \n",
    "    return len(content_words) / len(words)\n",
    "\n",
    "\n",
    "def indecisive_word_index(text):\n",
    "    \"\"\"Calculate indecisive word index (hedging vs assertive language)\"\"\"\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    hedging_words = {'possibly', 'perhaps', 'might', 'could', 'may', 'seems', 'appears', 'suggests', 'indicates', 'likely'}\n",
    "    assertive_words = {'definitely', 'certainly', 'clearly', 'obviously', 'undoubtedly', 'proves', 'demonstrates', 'shows'}\n",
    "    booster_words = {'very', 'extremely', 'highly', 'strongly', 'significantly'}\n",
    "    \n",
    "    if not words:\n",
    "        return 0\n",
    "    \n",
    "    # Position-weighted calculation\n",
    "    score = 0\n",
    "    for i, word in enumerate(words):\n",
    "        position_weight = 1 - (i / len(words))\n",
    "        if word in hedging_words:\n",
    "            score += 1 * position_weight\n",
    "        elif word in assertive_words:\n",
    "            score -= 1 * position_weight\n",
    "        elif word in booster_words:\n",
    "            score -= 0.5 * position_weight\n",
    "    \n",
    "    return score / len(words)\n",
    "\n",
    "fog = gunning_fog_index(cleaned_text)\n",
    "ari = automated_readability_index(cleaned_text)\n",
    "lex_density = calculate_lexical_density(cleaned_text)\n",
    "indecisive_idx = indecisive_word_index(cleaned_text)\n",
    "\n",
    "print(f\"\\n  Gunning Fog Index: {fog:.2f}\")\n",
    "print(f\"  Automated Readability Index: {ari:.2f}\")\n",
    "print(f\"  Lexical Density: {lex_density:.4f}\")\n",
    "if(indecisive_idx<0):\n",
    "    print(f\"  Indecisive Word Index: {0}\")\n",
    "else:\n",
    "    print(f\"  Indecisive Word Index: {indecisive_idx:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

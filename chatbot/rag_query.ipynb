{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b18c5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from collections import Counter\n",
    "import re\n",
    "from sentence_transformers import CrossEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00caee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"vector_july22\")\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"rag-chunks\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b17232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this import at the top of the cell\n",
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Initialize reranker globally\n",
    "reranker = None\n",
    "\n",
    "cs_seed_phrases = [\n",
    "    \"computer science\",\n",
    "    \"software engineering\",\n",
    "    \"algorithms\",\n",
    "    \"data structures\",\n",
    "    \"operating systems\",\n",
    "    \"computer networks\",\n",
    "    \"machine learning\",\n",
    "    \"artificial intelligence\",\n",
    "    \"database systems\",\n",
    "    \"distributed systems\",\n",
    "    \"computer architecture\",\n",
    "    \"theory of computation\",\n",
    "    \"programming languages\",\n",
    "    \"cybersecurity\",\n",
    "    \"information technology\"\n",
    "]\n",
    "cs_seed_embeddings = embedding_model.encode(cs_seed_phrases, normalize_embeddings=True)\n",
    "\n",
    "def is_computer_science_question(question: str, threshold: float = 0.55) -> bool:\n",
    "    \"\"\"Return True when the question is related to the Computer Science domain.\"\"\"\n",
    "    question_emb = embedding_model.encode([question], normalize_embeddings=True)[0]\n",
    "    similarities = cs_seed_embeddings @ question_emb\n",
    "    max_similarity = float(np.max(similarities))\n",
    "    print(f\" Domain similarity score: {max_similarity:.3f}\")\n",
    "    return max_similarity >= threshold\n",
    "\n",
    "def initialize_reranker():\n",
    "    global reranker\n",
    "    reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    return reranker\n",
    "\n",
    "def search_similar_chunks(query, n_results=5, filter_dict=None, min_relevance=0.5):\n",
    "    print(f\" Searching for: '{query}'\")\n",
    "    \n",
    "    try:\n",
    "        # Generate embedding for query with normalization\n",
    "        query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n",
    "        \n",
    "        # Get more candidates for reranking (3x the requested amount)\n",
    "        search_params = {\n",
    "            \"query_embeddings\": query_embedding.tolist(),\n",
    "            \"n_results\": min(n_results * 4, 40)  # Get up to 40 candidates\n",
    "        }\n",
    "        \n",
    "        if filter_dict:\n",
    "            search_params[\"where\"] = filter_dict\n",
    "            print(f\"   With filters: {filter_dict}\")\n",
    "        \n",
    "        # Search in collection\n",
    "        results = collection.query(**search_params)\n",
    "        \n",
    "        if not results['documents'][0]:\n",
    "            print(\"No results found\")\n",
    "            return []\n",
    "        \n",
    "        # Prepare candidates for reranking\n",
    "        candidates = []\n",
    "        for i, doc in enumerate(results['documents'][0]):\n",
    "            candidates.append({\n",
    "                'document': doc,\n",
    "                'metadata': results['metadatas'][0][i],\n",
    "                'initial_score': 1 - results['distances'][0][i],\n",
    "                'id': results['ids'][0][i]\n",
    "            })\n",
    "        \n",
    "        # Initialize reranker if not already done\n",
    "        global reranker\n",
    "        if reranker is None:\n",
    "            initialize_reranker()\n",
    "        \n",
    "        # Rerank using cross-encoder\n",
    "        print(f\"Reranking {len(candidates)} candidates...\")\n",
    "        pairs = [[query, c['document'][:512]] for c in candidates]  # Limit text for reranker\n",
    "        rerank_scores = reranker.predict(pairs)\n",
    "        \n",
    "        # Combine scores and filter\n",
    "        final_results = []\n",
    "        for i, candidate in enumerate(candidates):\n",
    "            # Combine initial embedding score with rerank score\n",
    "            combined_score = (candidate['initial_score'] * 0.3) + (rerank_scores[i] * 0.7)\n",
    "            \n",
    "            if combined_score >= min_relevance:\n",
    "                candidate['rerank_score'] = rerank_scores[i]\n",
    "                candidate['final_score'] = combined_score\n",
    "                final_results.append(candidate)\n",
    "        \n",
    "        # Sort by final score\n",
    "        final_results.sort(key=lambda x: x['final_score'], reverse=True)\n",
    "        top_results = final_results[:n_results]\n",
    "        \n",
    "        if not top_results:\n",
    "            print(f\"No results above relevance threshold {min_relevance}\")\n",
    "            return []\n",
    "        \n",
    "        print(f\" Found {len(candidates)} candidates, {len(final_results)} above threshold, returning top {len(top_results)}:\")\n",
    "        \n",
    "        for i, result in enumerate(top_results):\n",
    "            metadata = result['metadata']\n",
    "            print(f\"\\n{i+1}. {metadata['source']} (ID: {result['id'][-8:]})\") \n",
    "            if metadata.get('chapter'):\n",
    "                print(f\" Chapter {metadata['chapter']}\")\n",
    "            if metadata.get('section'):\n",
    "                print(f\"  Section {metadata['section']}\")\n",
    "            print(f\"   Initial: {result['initial_score']:.3f}, Rerank: {result['rerank_score']:.3f}, Final: {result['final_score']:.3f}\")\n",
    "            print(f\"   Chunk size: {len(result['document'])} chars\")\n",
    "            print(f\"   Preview: {result['document'][:200]}...\")\n",
    "        \n",
    "        return top_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\" Search failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "# Update build_context_for_chatbot to handle no results better\n",
    "def build_context_for_chatbot(query, n_chunks=3, min_relevance=0.5):\n",
    "    \"\"\"Build context for chatbot from search results\"\"\"\n",
    "    results = search_similar_chunks(query, n_chunks, min_relevance=min_relevance)\n",
    "    \n",
    "    if not results:\n",
    "        return \"I couldn't find sufficiently relevant information in the documents to answer your question. Please try rephrasing your query or ask about topics covered in the uploaded documents.\"\n",
    "    \n",
    "    # Only use results with high confidence\n",
    "    high_confidence_results = [r for r in results if r['final_score'] >= 0.7]\n",
    "    \n",
    "    if high_confidence_results:\n",
    "        context = \"Based on the following highly relevant information from the documents:\\n\\n\"\n",
    "    else:\n",
    "        context = \"Based on the following potentially relevant information from the documents (moderate confidence):\\n\\n\"\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        metadata = result['metadata']\n",
    "        context += f\"[Source {i+1}: {metadata['source']}\"\n",
    "        \n",
    "        if metadata.get('chapter'):\n",
    "            context += f\", Chapter {metadata['chapter']}\"\n",
    "        if metadata.get('section'):\n",
    "            context += f\", Section {metadata['section']}\"\n",
    "        \n",
    "        context += f\" - Relevance: {result['final_score']:.2f}]\\n\"\n",
    "        context += f\"{result['document']}\\n\\n\"\n",
    "        context += \"-\" * 50 + \"\\n\\n\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "def get_database_stats():\n",
    "    \"\"\"Get comprehensive database statistics\"\"\"\n",
    "    try:\n",
    "        count = collection.count()\n",
    "        print(f\"üìä Database Statistics:\")\n",
    "        print(f\"  üíæ Total chunks stored: {count}\")\n",
    "        \n",
    "        if count > 0:\n",
    "            # Get sample to analyze\n",
    "            sample = collection.get(limit=min(count, 1000))\n",
    "            \n",
    "            # Count chunks per document\n",
    "            source_counts = {}\n",
    "            chapter_counts = {}\n",
    "            \n",
    "            for meta in sample['metadatas']:\n",
    "                source = meta['source']\n",
    "                source_counts[source] = source_counts.get(source, 0) + 1\n",
    "                \n",
    "                if meta.get('chapter'):\n",
    "                    chapter_key = f\"{source} - Chapter {meta['chapter']}\"\n",
    "                    chapter_counts[chapter_key] = chapter_counts.get(chapter_key, 0) + 1\n",
    "            \n",
    "            print(f\"  Documents stored: {len(source_counts)}\")\n",
    "            print(f\"   Chapters identified: {len(chapter_counts)}\")\n",
    "            print(f\"\\n  Chunks per document:\")\n",
    "            for source, chunk_count in source_counts.items():\n",
    "                print(f\"     ‚Ä¢ {source}: {chunk_count} chunks\")\n",
    "        \n",
    "        return count\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting database stats: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "def show_sample_chunks(n_samples=3):\n",
    "    \"\"\"Show sample chunks from the database\"\"\"\n",
    "    try:\n",
    "        sample = collection.peek(limit=n_samples)\n",
    "        print(f\"Sample chunks (showing {n_samples}):\")\n",
    "        \n",
    "        for i, doc in enumerate(sample['documents']):\n",
    "            metadata = sample['metadatas'][i]\n",
    "            print(f\"\\n{i+1}. {metadata['source']} (Chunk {metadata['chunk_id']})\")\n",
    "            if metadata.get('chapter'):\n",
    "                print(f\"   Chapter {metadata['chapter']}\")\n",
    "            if metadata.get('section'):\n",
    "                print(f\"   Section {metadata['section']}\")\n",
    "            print(f\"   Content: {doc[:200]}...\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\" Error showing samples: {e}\")\n",
    "\n",
    "def answer_question(question, n_chunks=3, verbose=False):\n",
    "    \"\"\"\n",
    "    Simple Q&A function for chatbot integration\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"ü§î Question: {question}\")\n",
    "    \n",
    "    # Get context\n",
    "    context = build_context_for_chatbot(question, n_chunks)\n",
    "    \n",
    "    # Format for LLM\n",
    "    prompt = f\"\"\"Based on the following context from technical documents, please answer the question.\n",
    "    \n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüìù Context has {len(context.split())} words\")\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Keep the other utility functions as they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd83d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: test_qa() or chat_with_rag()\n"
     ]
    }
   ],
   "source": [
    "# Cell 19: Ollama LLM Integration\n",
    "import ollama\n",
    "\n",
    "'''def answer_with_ollama(question, n_chunks=3, model=\"llama3.1:8b-instruct-q4_0\", temperature=0.3):\n",
    "    \"\"\"\n",
    "    Answer questions using local Llama 3.1 with RAG context\n",
    "    \"\"\"\n",
    "    print(f\"ü§î Processing question: {question}\")\n",
    "    \n",
    "    # Get context from RAG\n",
    "    context = build_context_for_chatbot(question, n_chunks=n_chunks, min_relevance=0.3)\n",
    "    \n",
    "    # Check if we found relevant context\n",
    "    if \"couldn't find sufficiently relevant\" in context:\n",
    "        return \"I don't have relevant information in the documents to answer this question. Please try rephrasing or ask about topics covered in the uploaded documents.\"\n",
    "    \n",
    "    # Simple prompt\n",
    "    prompt = f\"\"\"Based on the following context from research documents, please answer the question accurately.\n",
    "If the context doesn't contain enough information, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"üöÄ Running {model}...\")\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': 'user', 'content': prompt}\n",
    "            ],\n",
    "            options={\n",
    "                'temperature': temperature,\n",
    "                'num_predict': 500\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return response['message']['content']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        print(\"Make sure Ollama is running: 'ollama serve'\")\n",
    "        return f\"Error: {str(e)}\"'''\n",
    "\n",
    "def chat_with_rag():\n",
    "    \"\"\"\n",
    "    Simple interactive chat\n",
    "    \"\"\"\n",
    "    print(\"RAG Chatbot with Llama 3.1\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    # Check Ollama\n",
    "    try:\n",
    "        ollama.list()\n",
    "        print(\"Ollama is running!\\n\")\n",
    "    except:\n",
    "        print(\" Start Ollama first: run 'ollama serve' in terminal\")\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\n Your question: \").strip()\n",
    "        \n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\" thank you!\")\n",
    "            break\n",
    "        \n",
    "        if not question:\n",
    "            continue\n",
    "        \n",
    "        answer = answer_with_ollama(question)\n",
    "        print(\"\\nAnswer:\")\n",
    "        print(answer)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Quick test\n",
    "def answer_with_ollama(question, n_chunks=3, model=\"llama3.1:8b-instruct-q4_0\", temperature=0.3):\n",
    "    \"\"\"\n",
    "    Answer questions using local Llama 3.1 with RAG context.\n",
    "    Refuses non-CS questions, falls back to pretrained CS knowledge when no supporting chunks are found.\n",
    "    \"\"\"\n",
    "    print(f\" Processing question: {question}\")\n",
    "    \n",
    "    # if not is_computer_science_question(question):\n",
    "    #     print(\" Question rejected: outside Computer Science domain.\")\n",
    "    #     return (\n",
    "    #         \"I only answer questions that relate to the Computer Science documents. \"\n",
    "    #         \"Please ask about Computer Science topics.\"\n",
    "    #     )\n",
    "    \n",
    "    # Get context from RAG\n",
    "    context = build_context_for_chatbot(question, n_chunks=n_chunks, min_relevance=0.3)\n",
    "    \n",
    "    # Decide how to craft the downstream prompt\n",
    "    if (\n",
    "        context is None\n",
    "        or context == \"NO_RELEVANT_DOCS_FOUND\"\n",
    "        or context.strip().startswith(\"I couldn't find sufficiently relevant information\")\n",
    "    ):\n",
    "        print(\" No relevant chunks found. Falling back to pretrained Computer Science knowledge.\")\n",
    "        fallback_context = (\n",
    "            \"No relevant document context was retrieved. Provide an answer using your general \"\n",
    "            \"Computer Science knowledge.\"\n",
    "        )\n",
    "        prompt = f\"\"\"Answer strictly from the Computer Science context below.\n",
    "If the question falls outside CS, reply that you cannot help.\n",
    "\n",
    "Context:\n",
    "{fallback_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    else:\n",
    "        import re\n",
    "        pdf_sources = re.findall(r'\\[Source \\d+: ([^,\\]]+)', context)\n",
    "        unique_pdfs = list(set(pdf_sources))\n",
    "        \n",
    "        prompt = f\"\"\"Based on the following context from research documents, please answer the question accurately.\n",
    "IMPORTANT: Always mention which PDF document(s) you are getting the information from (the PDFs are: {', '.join(unique_pdfs)}).\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer (make sure to mention which PDF the information comes from):\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\" Running {model}...\")\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': 'user', 'content': prompt}\n",
    "            ],\n",
    "            options={\n",
    "                'temperature': temperature,\n",
    "                'num_predict': 500\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return response['message']['content']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error: {e}\")\n",
    "        print(\"Make sure Ollama is running: 'ollama serve'\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "def test_qa():\n",
    "    \"\"\"Test with a simple question\"\"\"\n",
    "    question = \"What methodologies are discussed for plant disease detection?\"\n",
    "    print(f\" Test Question: {question}\\n\")\n",
    "    \n",
    "    answer = answer_with_ollama(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "print(\"Usage: test_qa() or chat_with_rag()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66fef6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved count: 3531\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieved count:\", collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30af1513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Total documents in collection: 3531\n"
     ]
    }
   ],
   "source": [
    "print(f\"üì¶ Total documents in collection: {collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9661996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing question: What is the methodology for plant disease detection?\n",
      " Searching for: 'What is the methodology for plant disease detection?'\n",
      "Reranking 12 candidates...\n",
      " Found 12 candidates, 10 above threshold, returning top 3:\n",
      "\n",
      "1. 8_Report - NIKHIL SHAJI.pdf (ID: _chunk_2)\n",
      " Chapter 2\n",
      "  Section 3.1\n",
      "   Initial: 0.786, Rerank: 7.044, Final: 5.166\n",
      "   Chunk size: 2603 chars\n",
      "   Preview: 12 of 46\n",
      "CHAPTER 2\n",
      "Problem Definition\n",
      "The existing manual methods for plant disease detection in agriculture are inefficient and prone to\n",
      "errors, leading to significant loss of crops and are a huge co...\n",
      "\n",
      "2. 8_Report - NIKHIL SHAJI.pdf (ID: chunk_14)\n",
      " Chapter 6\n",
      "  Section 6.1\n",
      "   Initial: 0.766, Rerank: 6.551, Final: 4.815\n",
      "   Chunk size: 1723 chars\n",
      "   Preview: 28 of 46\n",
      "CHAPTER 6\n",
      "System Design\n",
      "6.1 Current System\n",
      "The systems that exist for plant disease detection mainly follow a two-step process which is to identify\n",
      "if the plant is healthy or unhealthy using ...\n",
      "\n",
      "3. 8_Report - NIKHIL SHAJI.pdf (ID: chunk_10)\n",
      " Chapter 5\n",
      "  Section 5.1\n",
      "   Initial: 0.758, Rerank: 5.228, Final: 3.887\n",
      "   Chunk size: 1727 chars\n",
      "   Preview: 23 of 46\n",
      "CHAPTER 5\n",
      "Systems Requirements Specification\n",
      "5.1 Product Perspective\n",
      "5.1.1 Product Features\n",
      "1) Automated Disease Detection: Our plant disease detection model automatically detects plant\n",
      "disea...\n",
      " Running llama3.1:8b-instruct-q4_0...\n",
      "\n",
      " Answer:\n",
      "According to Chapter 2, Section 3.1 of the document \"8_Report - NIKHIL SHAJI.pdf\", the methodology for plant disease detection involves using deep learning frameworks, image processing techniques, and Generative Adversarial Networks (GANs) to correctly classify plant diseases from leaf images.\n",
      "\n",
      "Additionally, as mentioned in Chapter 6, Section 6.1 of the same document, the current system design follows a two-step process:\n",
      "\n",
      "1. Identifying if the plant is healthy or unhealthy using plant images.\n",
      "2. If the plant is found unhealthy, further classifying the specific disease which the plant is suffering from.\n",
      "\n",
      "The proposed methodology also includes using clustering algorithms and GANs to increase the dataset and improve the accuracy of disease detection.\n",
      "\n",
      "Furthermore, as mentioned in Chapter 5, Section 5.1 of the document, the product features include:\n",
      "\n",
      "* Automated Disease Detection: Using advanced image processing algorithms to automatically detect plant diseases.\n",
      "* GAN Integration: Using GANs for the generation of synthetic images that are close to real data to expand the training dataset and improve model performance.\n",
      "\n",
      "Overall, the methodology for plant disease detection involves a combination of deep learning frameworks, image processing techniques, clustering algorithms, and GANs to accurately classify plant diseases from leaf images.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the methodology for plant disease detection?\"\n",
    "answer = answer_with_ollama(question)\n",
    "print(\"\\n Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bd3504a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection name: rag-chunks\n",
      "Total documents: 3531\n",
      "Client type: <class 'chromadb.api.client.Client'>\n",
      "DB location (guess): C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages\\chromadb\\api\\client.py\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(\"Collection name:\", collection.name)\n",
    "print(\"Total documents:\", collection.count())\n",
    "\n",
    "# Inspect client object to confirm it's a PersistentClient\n",
    "print(\"Client type:\", type(chroma_client))\n",
    "\n",
    "# Optional: Look into where it's storing (if PersistentClient)\n",
    "print(\"DB location (guess):\", inspect.getsourcefile(type(chroma_client)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8d98f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing question: What is the approach to optimize task offloading and scheduling in fog computing\n",
      " Searching for: 'What is the approach to optimize task offloading and scheduling in fog computing'\n",
      "Reranking 12 candidates...\n",
      " Found 12 candidates, 11 above threshold, returning top 3:\n",
      "\n",
      "1. 46_Report - Meghana M.pdf (ID: _chunk_0)\n",
      "  Section 5.1\n",
      "   Initial: 0.884, Rerank: 6.973, Final: 5.146\n",
      "   Chunk size: 1680 chars\n",
      "   Preview: TABLE OF CONTENTS\n",
      "Chapter No.\n",
      "Title\n",
      "Page\n",
      "No.\n",
      "1.\n",
      "INTRODUCTION\n",
      "01\n",
      "2.\n",
      "PROBLEM DEFINITION\n",
      "02\n",
      "3.\n",
      "LITERATURE SURVEY\n",
      "03\n",
      "3.1.\n",
      "Computation Offloading and Task Scheduling Based on Improved\n",
      "Integer Particle Swar...\n",
      "\n",
      "2. 46_Report - Meghana M.pdf (ID: chunk_19)\n",
      " Chapter 5\n",
      "  Section 5.1\n",
      "   Initial: 0.865, Rerank: 6.914, Final: 5.099\n",
      "   Chunk size: 1529 chars\n",
      "   Preview: Jan-May-2021\n",
      "Page No.21\n",
      "CHAPTER 5\n",
      "SYSTEM REQUIREMENTS SPECIFICATION\n",
      "5.1 Introduction:\n",
      "Our project's main objective is to create methods that improve the effectiveness of task offloading\n",
      "and scheduling...\n",
      "\n",
      "3. 46_Report - Meghana M.pdf (ID: _chunk_3)\n",
      " Chapter 2\n",
      "   Initial: 0.889, Rerank: 6.463, Final: 4.791\n",
      "   Chunk size: 1218 chars\n",
      "   Preview: Jan-May-2021\n",
      "Page No.2\n",
      "CHAPTER 2\n",
      "PROBLEM DEFINITION\n",
      "Fog computing is a new emerging technology which supports smart devices. Fog computing brings\n",
      "computation closer to where the data is being generate...\n",
      " Running llama3.1:8b-instruct-q4_0...\n",
      "\n",
      " Answer:\n",
      "Based on the provided context from research documents:\n",
      "\n",
      "The approach to optimize task offloading and scheduling in fog computing involves the following steps:\n",
      "\n",
      "1. A comprehensive study of simulation environments (Source 3: Chapter 2, Page No.2) [46_Report - Meghana M.pdf]\n",
      "2. Creating a three-layer cloud-fog-device topology in the chosen simulation environment (Source 2: Section 5.2, Page No.21) [46_Report - Meghana M.pdf]\n",
      "3. Exploring algorithms to optimize task scheduling and offloading, testing them, and making changes iteratively to improve performance (Source 2: Section 5.2, Page No.21) [46_Report - Meghana M.pdf]\n",
      "\n",
      "Additionally, the project aims to develop innovative algorithms for efficient task offloading and scheduling through simulation of a device-fog-cloud architecture (Source 3: Chapter 2, Page No.2) [46_Report - Meghana M.pdf].\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"What is the approach to optimize task offloading and scheduling in fog computing\"\n",
    "answer = answer_with_ollama(question)\n",
    "print(\"\\n Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "998f0c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing question: What is discussed about plant disease detection?\n",
      " Searching for: 'What is discussed about plant disease detection?'\n",
      "Reranking 12 candidates...\n",
      " Found 12 candidates, 8 above threshold, returning top 3:\n",
      "\n",
      "1. 8_Report - NIKHIL SHAJI.pdf (ID: chunk_14)\n",
      " Chapter 6\n",
      "  Section 6.1\n",
      "   Initial: 0.746, Rerank: 5.545, Final: 4.105\n",
      "   Chunk size: 1723 chars\n",
      "   Preview: 28 of 46\n",
      "CHAPTER 6\n",
      "System Design\n",
      "6.1 Current System\n",
      "The systems that exist for plant disease detection mainly follow a two-step process which is to identify\n",
      "if the plant is healthy or unhealthy using ...\n",
      "\n",
      "2. 8_Report - NIKHIL SHAJI.pdf (ID: _chunk_2)\n",
      " Chapter 2\n",
      "  Section 3.1\n",
      "   Initial: 0.746, Rerank: 5.426, Final: 4.022\n",
      "   Chunk size: 2603 chars\n",
      "   Preview: 12 of 46\n",
      "CHAPTER 2\n",
      "Problem Definition\n",
      "The existing manual methods for plant disease detection in agriculture are inefficient and prone to\n",
      "errors, leading to significant loss of crops and are a huge co...\n",
      "\n",
      "3. 8_Report - NIKHIL SHAJI.pdf (ID: chunk_10)\n",
      " Chapter 5\n",
      "  Section 5.1\n",
      "   Initial: 0.742, Rerank: 3.926, Final: 2.971\n",
      "   Chunk size: 1727 chars\n",
      "   Preview: 23 of 46\n",
      "CHAPTER 5\n",
      "Systems Requirements Specification\n",
      "5.1 Product Perspective\n",
      "5.1.1 Product Features\n",
      "1) Automated Disease Detection: Our plant disease detection model automatically detects plant\n",
      "disea...\n",
      " Running llama3.1:8b-instruct-q4_0...\n",
      "Based on the provided context, here's what is discussed about plant disease detection:\n",
      "\n",
      "The current systems for plant disease detection mainly follow a two-step process, using a Classification Machine Learning Model such as a Convolutional Neural Network to classify plant images into unhealthy and healthy, and then further classifying specific disease types if the plant is found unhealthy (Source 1: 8_Report - NIKHIL SHAJI.pdf, Chapter 6, Section 6.1).\n",
      "\n",
      "However, these current classification models have issues with legacy datasets and need a way to classify new diseases. To resolve this, the proposed design intends to use Clustering Algorithms and GAN (General Adversarial Network) to increase the dataset (Source 1: 8_Report - NIKHIL SHAJI.pdf, Chapter 6, Section 6.2).\n",
      "\n",
      "The objective of the project is to create a system that can accurately differentiate between healthy and diseased plants, which is very important if any new disease emerges (Source 2: 8_Report - NIKHIL SHAJI.pdf, Chapter 2, Section 3.1). The proposed system will use deep learning frameworks, image processing techniques, and Generative Adversarial Networks (GANs) to correctly classify plant diseases.\n",
      "\n",
      "Additionally, the project aims to provide farmers with a reliable tool to find plant diseases, which will help them in farming efficiently and lead to better crop production and food security (Source 2: 8_Report - NIKHIL SHAJI.pdf, Chapter 3).\n",
      "\n",
      "The proposed system also includes a novel approach to classify unrecognized images by using clustering algorithms and the help of industry experts (Source 2: 8_Report - NIKHIL SHAJI.pdf, Chapter 3).\n"
     ]
    }
   ],
   "source": [
    "answer = answer_with_ollama(\"What is discussed about plant disease detection?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "479e1212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing question: Can you suggest some good research ideas based on Gans and data generation\n",
      " Searching for: 'Can you suggest some good research ideas based on Gans and data generation'\n",
      "Reranking 12 candidates...\n",
      " Found 12 candidates, 1 above threshold, returning top 1:\n",
      "\n",
      "1. 111_REPORT - Srujan Vr.pdf (ID: _chunk_4)\n",
      "   Initial: 0.820, Rerank: 1.306, Final: 1.160\n",
      "   Chunk size: 2706 chars\n",
      "   Preview: Dept. of CSE\n",
      "Jan - May, 2024\n",
      "Page No.\n",
      "Potential Future Directions: However, the paper concentrates on a summary of the researches of\n",
      "GANs existing and identifies the areas can be researched further to...\n",
      " Running llama3.1:8b-instruct-q4_0...\n",
      "\n",
      " Answer:\n",
      "Based on the provided context from the PDF document \"111_REPORT - Srujan Vr.pdf\" with a relevance score of 1.16, here are some potential research ideas related to GANs and data generation:\n",
      "\n",
      "1. **Exploring the Effects of Various Generative Adversarial Networks Techniques on Image Generation**: This idea is already explored in the paper mentioned in the context (Source: [4] Exploring the Effects of Various Generative Adversarial Networks Techniques on Image Generation Zian Shi1,2 , Junyi Teng1,2, Shihao Zheng1,2, Kaifeng Guo1*) and involves investigating the performance of different GAN techniques on standard datasets like MNIST.\n",
      "\n",
      "However, here are some additional research ideas that can be suggested based on the context:\n",
      "\n",
      "2. **Investigating the Impact of Different Loss Functions on GAN Performance**: This study could explore the effects of various loss functions on the performance of GANs in generating high-quality images or data.\n",
      "3. **Developing a Framework for Choosing the Optimal GAN Model for Specific Datasets**: Building upon the research gap mentioned in the context, this project could focus on developing a framework or repository that helps individuals select the most suitable GAN model and architecture for specific datasets.\n",
      "4. **Exploring the Application of GANs in Real-World Scenarios**: This study could investigate the practical applications of GANs in real-world scenarios, such as image synthesis for medical imaging, data augmentation for natural language processing, or generating synthetic data for training machine learning models.\n",
      "5. **Investigating the Effects of Adversarial Attacks on GANs**: This research idea involves exploring the vulnerability of GANs to adversarial attacks and developing techniques to improve their robustness against such attacks.\n",
      "\n",
      "These ideas are based on the context provided from the PDF document \"111_REPORT - Srujan Vr.pdf\" with a relevance score of 1.16.\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you suggest some good research ideas based on Gans and data generation\"\n",
    "answer = answer_with_ollama(question)\n",
    "print(\"\\n Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "124f631a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST 1: Question outside PDF scope - Should use pretrained knowledge\n",
      "================================================================================\n",
      " Processing question: What is the capital of France?\n",
      " Searching for: 'What is the capital of France?'\n",
      "Reranking 12 candidates...\n",
      "No results above relevance threshold 0.3\n",
      " No relevant chunks found. Falling back to pretrained Computer Science knowledge.\n",
      " Running llama3.1:8b-instruct-q4_0...\n",
      "\n",
      "üìå Answer:\n",
      "I cannot provide information on a political topic such as the capital of France. Is there anything else I can help you with?\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Question about something NOT in your PDFs (should use pretrained knowledge)\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST 1: Question outside PDF scope - Should use pretrained knowledge\")\n",
    "print(\"=\" * 80)\n",
    "question1 = \"What is the capital of France?\"\n",
    "answer1 = answer_with_ollama(question1)\n",
    "print(\"\\nüìå Answer:\")\n",
    "print(answer1)\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a01de03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST 2: Question within PDF scope - Should use RAG with PDF sources\n",
      "================================================================================\n",
      " Processing question: What is the methodology for plant disease detection?\n",
      " Searching for: 'What is the methodology for plant disease detection?'\n",
      "Reranking 12 candidates...\n",
      " Found 12 candidates, 10 above threshold, returning top 3:\n",
      "\n",
      "1. 8_Report - NIKHIL SHAJI.pdf (ID: _chunk_2)\n",
      " Chapter 2\n",
      "  Section 3.1\n",
      "   Initial: 0.786, Rerank: 7.044, Final: 5.166\n",
      "   Chunk size: 2603 chars\n",
      "   Preview: 12 of 46\n",
      "CHAPTER 2\n",
      "Problem Definition\n",
      "The existing manual methods for plant disease detection in agriculture are inefficient and prone to\n",
      "errors, leading to significant loss of crops and are a huge co...\n",
      "\n",
      "2. 8_Report - NIKHIL SHAJI.pdf (ID: chunk_14)\n",
      " Chapter 6\n",
      "  Section 6.1\n",
      "   Initial: 0.766, Rerank: 6.551, Final: 4.815\n",
      "   Chunk size: 1723 chars\n",
      "   Preview: 28 of 46\n",
      "CHAPTER 6\n",
      "System Design\n",
      "6.1 Current System\n",
      "The systems that exist for plant disease detection mainly follow a two-step process which is to identify\n",
      "if the plant is healthy or unhealthy using ...\n",
      "\n",
      "3. 8_Report - NIKHIL SHAJI.pdf (ID: chunk_10)\n",
      " Chapter 5\n",
      "  Section 5.1\n",
      "   Initial: 0.758, Rerank: 5.228, Final: 3.887\n",
      "   Chunk size: 1727 chars\n",
      "   Preview: 23 of 46\n",
      "CHAPTER 5\n",
      "Systems Requirements Specification\n",
      "5.1 Product Perspective\n",
      "5.1.1 Product Features\n",
      "1) Automated Disease Detection: Our plant disease detection model automatically detects plant\n",
      "disea...\n",
      " Running llama3.1:8b-instruct-q4_0...\n",
      "\n",
      "üìå Answer:\n",
      "According to Chapter 2, Section 3.1 of the document \"8_Report - NIKHIL SHAJI.pdf\" (Source 1), the methodology for plant disease detection involves using deep learning frameworks, image processing techniques, and Generative Adversarial Networks (GANs) to correctly classify plant diseases from leaf images.\n",
      "\n",
      "Additionally, Chapter 6, Section 6.1 of the same document (Source 2) mentions that the current system design uses a two-step process:\n",
      "\n",
      "1. Identifying if the plant is healthy or unhealthy using plant images.\n",
      "2. If the plant is found unhealthy, further classifying the specific disease type using a Classification Machine Learning Model such as a Convolutional Neural Network.\n",
      "\n",
      "The proposed methodology also includes using clustering algorithms and GANs to increase the dataset and improve the accuracy of disease detection (Chapter 6, Section 6.1).\n",
      "\n",
      "Lastly, Chapter 5, Section 5.1 of the document (Source 3) mentions that the plant disease detection model uses advanced image processing algorithms for automated disease detection and integrates GANs for generating synthetic images to expand the training dataset.\n",
      "\n",
      "Therefore, the methodology for plant disease detection involves a combination of deep learning frameworks, image processing techniques, GANs, clustering algorithms, and Classification Machine Learning Models.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Question about something IN your PDFs (should use RAG context)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 2: Question within PDF scope - Should use RAG with PDF sources\")\n",
    "print(\"=\" * 80)\n",
    "question2 = \"What is the methodology for plant disease detection?\"\n",
    "answer2 = answer_with_ollama(question2)\n",
    "print(\"\\nüìå Answer:\")\n",
    "print(answer2)\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a09199d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Testing suite ready!\n",
      "Run cells 11 and 12 above to see the fallback functionality in action.\n",
      "\n",
      "Make sure Ollama is running: 'ollama serve' in terminal\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INSTRUCTIONS TO TEST THE NEW FALLBACK FUNCTIONALITY\n",
    "# ============================================================================\n",
    "# \n",
    "# 1. First, run cells 1-3 to initialize everything (imports, models, functions)\n",
    "# 2. Then run the test cells above (cells 11 & 12) to see:\n",
    "#    - Test 1: Model using pretrained knowledge (no PDFs mentioned)\n",
    "#    - Test 2: Model using RAG context (PDF sources mentioned)\n",
    "#\n",
    "# Key behaviors:\n",
    "# ‚úÖ When chunks ARE found: Model answers from PDFs and mentions sources\n",
    "# ‚úÖ When chunks are NOT found: Model answers from general knowledge with disclaimer\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìö Testing suite ready!\")\n",
    "print(\"Run cells 11 and 12 above to see the fallback functionality in action.\")\n",
    "print(\"\\nMake sure Ollama is running: 'ollama serve' in terminal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a27a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

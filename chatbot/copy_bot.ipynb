{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "919b7f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from collections import Counter\n",
    "import re\n",
    "from sentence_transformers import CrossEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943f5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"vector_july22\")\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"rag-chunks\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be673d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this import at the top of the cell\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Initialize reranker globally\n",
    "reranker = None\n",
    "\n",
    "def initialize_reranker():\n",
    "    global reranker\n",
    "    reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    return reranker\n",
    "\n",
    "def search_similar_chunks(query, n_results=5, filter_dict=None, min_relevance=0.5):\n",
    "    print(f\" Searching for: '{query}'\")\n",
    "    \n",
    "    try:\n",
    "        # Generate embedding for query with normalization\n",
    "        query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n",
    "        \n",
    "        # Get more candidates for reranking (3x the requested amount)\n",
    "        search_params = {\n",
    "            \"query_embeddings\": query_embedding.tolist(),\n",
    "            \"n_results\": min(n_results * 4, 40)  # Get up to 40 candidates\n",
    "        }\n",
    "        \n",
    "        if filter_dict:\n",
    "            search_params[\"where\"] = filter_dict\n",
    "            print(f\"   With filters: {filter_dict}\")\n",
    "        \n",
    "        # Search in collection\n",
    "        results = collection.query(**search_params)\n",
    "        \n",
    "        if not results['documents'][0]:\n",
    "            print(\"No results found\")\n",
    "            return []\n",
    "        \n",
    "        # Prepare candidates for reranking\n",
    "        candidates = []\n",
    "        for i, doc in enumerate(results['documents'][0]):\n",
    "            candidates.append({\n",
    "                'document': doc,\n",
    "                'metadata': results['metadatas'][0][i],\n",
    "                'initial_score': 1 - results['distances'][0][i],\n",
    "                'id': results['ids'][0][i]\n",
    "            })\n",
    "        \n",
    "        # Initialize reranker if not already done\n",
    "        global reranker\n",
    "        if reranker is None:\n",
    "            initialize_reranker()\n",
    "        \n",
    "        # Rerank using cross-encoder\n",
    "        print(f\"Reranking {len(candidates)} candidates...\")\n",
    "        pairs = [[query, c['document'][:512]] for c in candidates]  # Limit text for reranker\n",
    "        rerank_scores = reranker.predict(pairs)\n",
    "        \n",
    "        # Combine scores and filter\n",
    "        final_results = []\n",
    "        for i, candidate in enumerate(candidates):\n",
    "            # Combine initial embedding score with rerank score\n",
    "            combined_score = (candidate['initial_score'] * 0.3) + (rerank_scores[i] * 0.7)\n",
    "            \n",
    "            if combined_score >= min_relevance:\n",
    "                candidate['rerank_score'] = rerank_scores[i]\n",
    "                candidate['final_score'] = combined_score\n",
    "                final_results.append(candidate)\n",
    "        \n",
    "        # Sort by final score\n",
    "        final_results.sort(key=lambda x: x['final_score'], reverse=True)\n",
    "        top_results = final_results[:n_results]\n",
    "        \n",
    "        if not top_results:\n",
    "            print(f\"No results above relevance threshold {min_relevance}\")\n",
    "            return []\n",
    "        \n",
    "        print(f\" Found {len(candidates)} candidates, {len(final_results)} above threshold, returning top {len(top_results)}:\")\n",
    "        \n",
    "        for i, result in enumerate(top_results):\n",
    "            metadata = result['metadata']\n",
    "            print(f\"\\n{i+1}. {metadata['source']} (ID: {result['id'][-8:]})\") \n",
    "            if metadata.get('chapter'):\n",
    "                print(f\" Chapter {metadata['chapter']}\")\n",
    "            if metadata.get('section'):\n",
    "                print(f\"  Section {metadata['section']}\")\n",
    "            print(f\"   Initial: {result['initial_score']:.3f}, Rerank: {result['rerank_score']:.3f}, Final: {result['final_score']:.3f}\")\n",
    "            print(f\"   Chunk size: {len(result['document'])} chars\")\n",
    "            print(f\"   Preview: {result['document'][:200]}...\")\n",
    "        \n",
    "        return top_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\" Search failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "# Update build_context_for_chatbot to handle no results better\n",
    "def build_context_for_chatbot(query, n_chunks=3, min_relevance=0.5):\n",
    "    \"\"\"Build context for chatbot from search results\"\"\"\n",
    "    results = search_similar_chunks(query, n_chunks, min_relevance=min_relevance)\n",
    "    \n",
    "    if not results:\n",
    "        return \"I couldn't find sufficiently relevant information in the documents to answer your question. Please try rephrasing your query or ask about topics covered in the uploaded documents.\"\n",
    "    \n",
    "    # Only use results with high confidence\n",
    "    high_confidence_results = [r for r in results if r['final_score'] >= 0.7]\n",
    "    \n",
    "    if high_confidence_results:\n",
    "        context = \"Based on the following highly relevant information from the documents:\\n\\n\"\n",
    "    else:\n",
    "        context = \"Based on the following potentially relevant information from the documents (moderate confidence):\\n\\n\"\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        metadata = result['metadata']\n",
    "        context += f\"[Source {i+1}: {metadata['source']}\"\n",
    "        \n",
    "        if metadata.get('chapter'):\n",
    "            context += f\", Chapter {metadata['chapter']}\"\n",
    "        if metadata.get('section'):\n",
    "            context += f\", Section {metadata['section']}\"\n",
    "        \n",
    "        context += f\" - Relevance: {result['final_score']:.2f}]\\n\"\n",
    "        context += f\"{result['document']}\\n\\n\"\n",
    "        context += \"-\" * 50 + \"\\n\\n\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "def get_database_stats():\n",
    "    \"\"\"Get comprehensive database statistics\"\"\"\n",
    "    try:\n",
    "        count = collection.count()\n",
    "        print(f\"üìä Database Statistics:\")\n",
    "        print(f\"  üíæ Total chunks stored: {count}\")\n",
    "        \n",
    "        if count > 0:\n",
    "            # Get sample to analyze\n",
    "            sample = collection.get(limit=min(count, 1000))\n",
    "            \n",
    "            # Count chunks per document\n",
    "            source_counts = {}\n",
    "            chapter_counts = {}\n",
    "            \n",
    "            for meta in sample['metadatas']:\n",
    "                source = meta['source']\n",
    "                source_counts[source] = source_counts.get(source, 0) + 1\n",
    "                \n",
    "                if meta.get('chapter'):\n",
    "                    chapter_key = f\"{source} - Chapter {meta['chapter']}\"\n",
    "                    chapter_counts[chapter_key] = chapter_counts.get(chapter_key, 0) + 1\n",
    "            \n",
    "            print(f\"  Documents stored: {len(source_counts)}\")\n",
    "            print(f\"   Chapters identified: {len(chapter_counts)}\")\n",
    "            print(f\"\\n  Chunks per document:\")\n",
    "            for source, chunk_count in source_counts.items():\n",
    "                print(f\"     ‚Ä¢ {source}: {chunk_count} chunks\")\n",
    "        \n",
    "        return count\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting database stats: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "def show_sample_chunks(n_samples=3):\n",
    "    \"\"\"Show sample chunks from the database\"\"\"\n",
    "    try:\n",
    "        sample = collection.peek(limit=n_samples)\n",
    "        print(f\"Sample chunks (showing {n_samples}):\")\n",
    "        \n",
    "        for i, doc in enumerate(sample['documents']):\n",
    "            metadata = sample['metadatas'][i]\n",
    "            print(f\"\\n{i+1}. {metadata['source']} (Chunk {metadata['chunk_id']})\")\n",
    "            if metadata.get('chapter'):\n",
    "                print(f\"   Chapter {metadata['chapter']}\")\n",
    "            if metadata.get('section'):\n",
    "                print(f\"   Section {metadata['section']}\")\n",
    "            print(f\"   Content: {doc[:200]}...\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\" Error showing samples: {e}\")\n",
    "\n",
    "def answer_question(question, n_chunks=3, verbose=False):\n",
    "    \"\"\"\n",
    "    Simple Q&A function for chatbot integration\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"ü§î Question: {question}\")\n",
    "    \n",
    "    # Get context\n",
    "    context = build_context_for_chatbot(question, n_chunks)\n",
    "    \n",
    "    # Format for LLM\n",
    "    prompt = f\"\"\"Based on the following context from technical documents, please answer the question.\n",
    "    \n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüìù Context has {len(context.split())} words\")\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Keep the other utility functions as they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6849668e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: test_qa() or chat_with_rag()\n"
     ]
    }
   ],
   "source": [
    "# Cell 19: Ollama LLM Integration\n",
    "import ollama\n",
    "\n",
    "'''def answer_with_ollama(question, n_chunks=3, model=\"llama3.1:8b-instruct-q4_0\", temperature=0.3):\n",
    "    \"\"\"\n",
    "    Answer questions using local Llama 3.1 with RAG context\n",
    "    \"\"\"\n",
    "    print(f\"ü§î Processing question: {question}\")\n",
    "    \n",
    "    # Get context from RAG\n",
    "    context = build_context_for_chatbot(question, n_chunks=n_chunks, min_relevance=0.3)\n",
    "    \n",
    "    # Check if we found relevant context\n",
    "    if \"couldn't find sufficiently relevant\" in context:\n",
    "        return \"I don't have relevant information in the documents to answer this question. Please try rephrasing or ask about topics covered in the uploaded documents.\"\n",
    "    \n",
    "    # Simple prompt\n",
    "    prompt = f\"\"\"Based on the following context from research documents, please answer the question accurately.\n",
    "If the context doesn't contain enough information, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"üöÄ Running {model}...\")\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': 'user', 'content': prompt}\n",
    "            ],\n",
    "            options={\n",
    "                'temperature': temperature,\n",
    "                'num_predict': 500\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return response['message']['content']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        print(\"Make sure Ollama is running: 'ollama serve'\")\n",
    "        return f\"Error: {str(e)}\"'''\n",
    "\n",
    "def chat_with_rag():\n",
    "    \"\"\"\n",
    "    Simple interactive chat\n",
    "    \"\"\"\n",
    "    print(\"RAG Chatbot with Llama 3.1\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    # Check Ollama\n",
    "    try:\n",
    "        ollama.list()\n",
    "        print(\"Ollama is running!\\n\")\n",
    "    except:\n",
    "        print(\" Start Ollama first: run 'ollama serve' in terminal\")\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\n Your question: \").strip()\n",
    "        \n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\" thank you!\")\n",
    "            break\n",
    "        \n",
    "        if not question:\n",
    "            continue\n",
    "        \n",
    "        answer = answer_with_ollama(question)\n",
    "        print(\"\\nAnswer:\")\n",
    "        print(answer)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Quick test\n",
    "def answer_with_ollama(question, n_chunks=3, model=\"llama3.1:8b-instruct-q4_0\", temperature=0.3):\n",
    "    \"\"\"\n",
    "    Answer questions using local Llama 3.1 with RAG context\n",
    "    Modified to ensure PDF sources are mentioned\n",
    "    \"\"\"\n",
    "    print(f\" Processing question: {question}\")\n",
    "    \n",
    "    # Get context from RAG\n",
    "    context = build_context_for_chatbot(question, n_chunks=n_chunks, min_relevance=0.3)\n",
    "    \n",
    "    # Check if we found relevant context\n",
    "    if \"couldn't find sufficiently relevant\" in context:\n",
    "        return \"I don't have relevant information in the documents to answer this question. Please try rephrasing or ask about topics covered in the uploaded documents.\"\n",
    "    \n",
    "    # Extract PDF names from context\n",
    "    import re\n",
    "    pdf_sources = re.findall(r'\\[Source \\d+: ([^,\\]]+)', context)\n",
    "    unique_pdfs = list(set(pdf_sources))\n",
    "    \n",
    "    # Modified prompt that explicitly asks to mention PDF sources\n",
    "    prompt = f\"\"\"Based on the following context from research documents, please answer the question accurately.\n",
    "IMPORTANT: Always mention which PDF document(s) you are getting the information from (the PDFs are: {', '.join(unique_pdfs)}).\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer (make sure to mention which PDF the information comes from):\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\" Running {model}...\")\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': 'user', 'content': prompt}\n",
    "            ],\n",
    "            options={\n",
    "                'temperature': temperature,\n",
    "                'num_predict': 500\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return response['message']['content']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error: {e}\")\n",
    "        print(\"Make sure Ollama is running: 'ollama serve'\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "def test_qa():\n",
    "    \"\"\"Test with a simple question\"\"\"\n",
    "    question = \"What methodologies are discussed for plant disease detection?\"\n",
    "    print(f\" Test Question: {question}\\n\")\n",
    "    \n",
    "    answer = answer_with_ollama(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "print(\"Usage: test_qa() or chat_with_rag()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9d432ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved count: 3531\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieved count:\", collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b158c3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Total documents in collection: 3531\n"
     ]
    }
   ],
   "source": [
    "print(f\"üì¶ Total documents in collection: {collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c669ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing question: What is the methodology for plant disease detection?\n",
      " Searching for: 'What is the methodology for plant disease detection?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jsdha\\.cache\\huggingface\\hub\\models--cross-encoder--ms-marco-MiniLM-L-6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranking 12 candidates...\n",
      " Found 12 candidates, 10 above threshold, returning top 3:\n",
      "\n",
      "1. 8_Report - NIKHIL SHAJI.pdf (ID: _chunk_2)\n",
      " Chapter 2\n",
      "  Section 3.1\n",
      "   Initial: 0.786, Rerank: 7.044, Final: 5.166\n",
      "   Chunk size: 2603 chars\n",
      "   Preview: 12 of 46\n",
      "CHAPTER 2\n",
      "Problem Definition\n",
      "The existing manual methods for plant disease detection in agriculture are inefficient and prone to\n",
      "errors, leading to significant loss of crops and are a huge co...\n",
      "\n",
      "2. 8_Report - NIKHIL SHAJI.pdf (ID: chunk_14)\n",
      " Chapter 6\n",
      "  Section 6.1\n",
      "   Initial: 0.766, Rerank: 6.551, Final: 4.815\n",
      "   Chunk size: 1723 chars\n",
      "   Preview: 28 of 46\n",
      "CHAPTER 6\n",
      "System Design\n",
      "6.1 Current System\n",
      "The systems that exist for plant disease detection mainly follow a two-step process which is to identify\n",
      "if the plant is healthy or unhealthy using ...\n",
      "\n",
      "3. 8_Report - NIKHIL SHAJI.pdf (ID: chunk_10)\n",
      " Chapter 5\n",
      "  Section 5.1\n",
      "   Initial: 0.758, Rerank: 5.228, Final: 3.887\n",
      "   Chunk size: 1727 chars\n",
      "   Preview: 23 of 46\n",
      "CHAPTER 5\n",
      "Systems Requirements Specification\n",
      "5.1 Product Perspective\n",
      "5.1.1 Product Features\n",
      "1) Automated Disease Detection: Our plant disease detection model automatically detects plant\n",
      "disea...\n",
      " Running llama3.1:8b-instruct-q4_0...\n",
      "\n",
      " Answer:\n",
      "According to Chapter 2, Section 3.1 of the document \"8_Report - NIKHIL SHAJI.pdf\", the methodology for plant disease detection involves using deep learning frameworks, image processing techniques, and Generative Adversarial Networks (GANs) to correctly classify plant diseases from leaf images.\n",
      "\n",
      "Additionally, as mentioned in Chapter 6, Section 6.1 of the same document, the current system design involves a two-step process:\n",
      "\n",
      "1. Identifying if the plant is healthy or unhealthy using plant images.\n",
      "2. If the plant is found unhealthy, further classifying the specific disease which the plant is suffering from.\n",
      "\n",
      "The proposed methodology also includes the use of clustering algorithms and GANs to increase the dataset and improve disease detection accuracy.\n",
      "\n",
      "Furthermore, as mentioned in Chapter 5, Section 5.1 of the document, the product features include:\n",
      "\n",
      "* Automated Disease Detection using advanced image processing algorithms.\n",
      "* GAN Integration for generating synthetic images that are close to real data.\n",
      "* Cloud Deployment for easy access by users from any location at any time.\n",
      "* User-Friendly Interface for easy operation without confusion.\n",
      "\n",
      "Overall, the methodology for plant disease detection involves a combination of deep learning frameworks, image processing techniques, and clustering algorithms, with a focus on improving accuracy and user experience.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the methodology for plant disease detection?\"\n",
    "answer = answer_with_ollama(question)\n",
    "print(\"\\n Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e69acf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection name: rag-chunks\n",
      "Total documents: 3531\n",
      "Client type: <class 'chromadb.api.client.Client'>\n",
      "DB location (guess): C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages\\chromadb\\api\\client.py\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(\"Collection name:\", collection.name)\n",
    "print(\"Total documents:\", collection.count())\n",
    "\n",
    "# Inspect client object to confirm it's a PersistentClient\n",
    "print(\"Client type:\", type(chroma_client))\n",
    "\n",
    "# Optional: Look into where it's storing (if PersistentClient)\n",
    "print(\"DB location (guess):\", inspect.getsourcefile(type(chroma_client)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4da5a034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing question: What is the approach to optimize task offloading and scheduling in fog computing\n",
      " Searching for: 'What is the approach to optimize task offloading and scheduling in fog computing'\n",
      "Reranking 12 candidates...\n",
      " Found 12 candidates, 11 above threshold, returning top 3:\n",
      "\n",
      "1. 46_Report - Meghana M.pdf (ID: _chunk_0)\n",
      "  Section 5.1\n",
      "   Initial: 0.884, Rerank: 6.973, Final: 5.146\n",
      "   Chunk size: 1680 chars\n",
      "   Preview: TABLE OF CONTENTS\n",
      "Chapter No.\n",
      "Title\n",
      "Page\n",
      "No.\n",
      "1.\n",
      "INTRODUCTION\n",
      "01\n",
      "2.\n",
      "PROBLEM DEFINITION\n",
      "02\n",
      "3.\n",
      "LITERATURE SURVEY\n",
      "03\n",
      "3.1.\n",
      "Computation Offloading and Task Scheduling Based on Improved\n",
      "Integer Particle Swar...\n",
      "\n",
      "2. 46_Report - Meghana M.pdf (ID: chunk_19)\n",
      " Chapter 5\n",
      "  Section 5.1\n",
      "   Initial: 0.865, Rerank: 6.914, Final: 5.099\n",
      "   Chunk size: 1529 chars\n",
      "   Preview: Jan-May-2021\n",
      "Page No.21\n",
      "CHAPTER 5\n",
      "SYSTEM REQUIREMENTS SPECIFICATION\n",
      "5.1 Introduction:\n",
      "Our project's main objective is to create methods that improve the effectiveness of task offloading\n",
      "and scheduling...\n",
      "\n",
      "3. 46_Report - Meghana M.pdf (ID: _chunk_3)\n",
      " Chapter 2\n",
      "   Initial: 0.889, Rerank: 6.463, Final: 4.791\n",
      "   Chunk size: 1218 chars\n",
      "   Preview: Jan-May-2021\n",
      "Page No.2\n",
      "CHAPTER 2\n",
      "PROBLEM DEFINITION\n",
      "Fog computing is a new emerging technology which supports smart devices. Fog computing brings\n",
      "computation closer to where the data is being generate...\n",
      " Running llama3.1:8b-instruct-q4_0...\n",
      "\n",
      " Answer:\n",
      "Based on the provided context from research documents:\n",
      "\n",
      "The approach to optimize task offloading and scheduling in fog computing involves the following steps, as mentioned in [Source 1: 46_Report - Meghana M.pdf, Section 5.1 - Relevance: 5.15]:\n",
      "\n",
      "1. **Comprehensive study of simulation environments**: This includes studying various simulation environments available to simulate fog environments and identifying the most suitable one for conducting the experiment effectively.\n",
      "2. **Creating Fog topology**: A three-layer cloud-fog-device topology will be created in the chosen simulation environment where algorithms will be deployed.\n",
      "3. **Usage of algorithms**: Algorithms will be explored, tested, and iteratively improved to optimize task scheduling and offloading.\n",
      "4. **Result analysis**: Results will be presented in terms of improvements achieved from existing approaches in numerical values and graphs.\n",
      "\n",
      "Additionally, as mentioned in [Source 2: 46_Report - Meghana M.pdf, Chapter 5, Section 5.1], the project's main objective is to create methods that improve the effectiveness of task offloading and scheduling procedures in fog computing settings by optimizing performance parameters like energy, latency, and makespan for a particular fog topology.\n",
      "\n",
      "Furthermore, as mentioned in [Source 3: 46_Report - Meghana M.pdf, Chapter 2], the project will simulate a device-fog-cloud architecture to develop and test innovative algorithms for efficient task offloading and scheduling.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"What is the approach to optimize task offloading and scheduling in fog computing\"\n",
    "answer = answer_with_ollama(question)\n",
    "print(\"\\n Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3414df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing question: What is discussed about plant disease detection?\n",
      " Searching for: 'What is discussed about plant disease detection?'\n",
      "Reranking 12 candidates...\n",
      " Found 12 candidates, 8 above threshold, returning top 3:\n",
      "\n",
      "1. 8_Report - NIKHIL SHAJI.pdf (ID: chunk_14)\n",
      " Chapter 6\n",
      "  Section 6.1\n",
      "   Initial: 0.746, Rerank: 5.545, Final: 4.105\n",
      "   Chunk size: 1723 chars\n",
      "   Preview: 28 of 46\n",
      "CHAPTER 6\n",
      "System Design\n",
      "6.1 Current System\n",
      "The systems that exist for plant disease detection mainly follow a two-step process which is to identify\n",
      "if the plant is healthy or unhealthy using ...\n",
      "\n",
      "2. 8_Report - NIKHIL SHAJI.pdf (ID: _chunk_2)\n",
      " Chapter 2\n",
      "  Section 3.1\n",
      "   Initial: 0.746, Rerank: 5.426, Final: 4.022\n",
      "   Chunk size: 2603 chars\n",
      "   Preview: 12 of 46\n",
      "CHAPTER 2\n",
      "Problem Definition\n",
      "The existing manual methods for plant disease detection in agriculture are inefficient and prone to\n",
      "errors, leading to significant loss of crops and are a huge co...\n",
      "\n",
      "3. 8_Report - NIKHIL SHAJI.pdf (ID: chunk_10)\n",
      " Chapter 5\n",
      "  Section 5.1\n",
      "   Initial: 0.742, Rerank: 3.926, Final: 2.971\n",
      "   Chunk size: 1727 chars\n",
      "   Preview: 23 of 46\n",
      "CHAPTER 5\n",
      "Systems Requirements Specification\n",
      "5.1 Product Perspective\n",
      "5.1.1 Product Features\n",
      "1) Automated Disease Detection: Our plant disease detection model automatically detects plant\n",
      "disea...\n",
      " Running llama3.1:8b-instruct-q4_0...\n",
      "Based on the provided context, here's what is discussed about plant disease detection:\n",
      "\n",
      "According to Chapter 6 of the report [Source 1: 8_Report - NIKHIL SHAJI.pdf], the current system for plant disease detection mainly follows a two-step process using a Classification Machine Learning Model such as a Convolutional Neural Network (CNN) to classify plant images into unhealthy and healthy, and then further classify specific diseases. However, this approach has limitations, including the use of legacy datasets that need updating.\n",
      "\n",
      "The report also mentions that one of the design goals is to enhance disease detection accuracy [Source 1: 8_Report - NIKHIL SHAJI.pdf, Chapter 6]. Additionally, it highlights the importance of improving user experience by providing a non-complex and intuitive interface for farmers to upload plant images and receive disease diagnosis.\n",
      "\n",
      "In Chapter 2 of the report [Source 2: 8_Report - NIKHIL SHAJI.pdf], it is stated that manual methods for plant disease detection are inefficient and prone to errors, leading to significant loss of crops. The objective is to create a system that can accurately differentiate between healthy and diseased plants.\n",
      "\n",
      "Furthermore, Chapter 3 of the report [Source 2: 8_Report - NIKHIL SHAJI.pdf] discusses a paper on Leafgan model, which proposes an image translation system using Generative Adversarial Networks (GANs) to generate synthetic images of diseased plants from healthy ones. However, it highlights limitations such as domain specificity and background complexity.\n",
      "\n",
      "Lastly, Chapter 5 of the report [Source 3: 8_Report - NIKHIL SHAJI.pdf] mentions that the plant disease detection model automatically detects diseases using advanced image processing algorithms, reducing the need for manual inspection and enabling early detection.\n"
     ]
    }
   ],
   "source": [
    "answer = answer_with_ollama(\"What is discussed about plant disease detection?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0623f0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chatbot with Llama 3.1\n",
      "Type 'quit' to exit\n",
      "\n",
      "Ollama is running!\n",
      "\n",
      " Processing question: how are gans used to detect a disease in plants\n",
      " Searching for: 'how are gans used to detect a disease in plants'\n",
      "Reranking 12 candidates...\n",
      " Found 12 candidates, 9 above threshold, returning top 3:\n",
      "\n",
      "1. 8_Report - NIKHIL SHAJI.pdf (ID: chunk_10)\n",
      " Chapter 5\n",
      "  Section 5.1\n",
      "   Initial: 0.786, Rerank: 7.438, Final: 5.442\n",
      "   Chunk size: 1727 chars\n",
      "   Preview: 23 of 46\n",
      "CHAPTER 5\n",
      "Systems Requirements Specification\n",
      "5.1 Product Perspective\n",
      "5.1.1 Product Features\n",
      "1) Automated Disease Detection: Our plant disease detection model automatically detects plant\n",
      "disea...\n",
      "\n",
      "2. 8_Report - NIKHIL SHAJI.pdf (ID: chunk_20)\n",
      "  Section 6.4\n",
      "   Initial: 0.850, Rerank: 6.171, Final: 4.574\n",
      "   Chunk size: 2981 chars\n",
      "   Preview: 35 of 46\n",
      "We aim to have an automated plant disease detection model while maintaining the accuracy of the\n",
      "prediction. The integration of GAN, the classification model, and the clustering model increase...\n",
      "\n",
      "3. 8_Report - NIKHIL SHAJI.pdf (ID: chunk_17)\n",
      "  Section 2.2\n",
      "   Initial: 0.798, Rerank: 5.551, Final: 4.125\n",
      "   Chunk size: 1543 chars\n",
      "   Preview: 31 of 46\n",
      "particular plant disease. This helps us in data\n",
      "augmentation which in turn makes the machine\n",
      "learning model more accurate.\n",
      "those which have less number of images in the\n",
      "training data and skew...\n",
      " Running llama3.1:8b-instruct-q4_0...\n",
      "\n",
      "Answer:\n",
      "According to the provided context, GANs (Generative Adversarial Networks) are used to detect diseases in plants by generating synthetic images that are really close to real data. This is stated in Chapter 5 of Section 1: \"Product Features\" in the document \"8_Report - NIKHIL SHAJI.pdf\".\n",
      "\n",
      "Specifically, it says:\n",
      "\n",
      "\"...we use GANs for the generation of synthetic images that are really close to the real data. By doing this we increase the performance and the efficiency of the model and also help in recognizing a whole wide variety of plant diseases.\"\n",
      "\n",
      "Additionally, Section 6.4 of the same document mentions that the integration of GAN with other models (classification and clustering) increases the likelihood of better disease classification and identification.\n",
      "\n",
      "Therefore, the answer to the question is:\n",
      "\n",
      "GANs are used to detect diseases in plants by generating synthetic images that help increase the performance and efficiency of the model, allowing for a wider recognition of plant diseases.\n",
      "\n",
      "==================================================\n",
      " thank you!\n"
     ]
    }
   ],
   "source": [
    "chat_with_rag()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "992a4dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pymupdf in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (1.25.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: chromadb in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (1.0.13)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (2.11.1)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (1.4.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (2.2.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (4.13.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (0.21.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (1.73.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (4.24.0)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from pydantic>=1.9->chromadb) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from tokenizers>=0.13.2->chromadb) (0.30.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf\n",
    "!pip install chromadb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8986a669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (0.30.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (11.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.13.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (77.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d69fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (0.3.26)\n",
      "Requirement already satisfied: chromadb in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (1.0.13)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (4.1.0)\n",
      "Requirement already satisfied: pypdf in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (5.6.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (0.3.66)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (0.4.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (2.11.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.13.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (1.4.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (2.2.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (0.21.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (1.73.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from chromadb) (4.24.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (0.30.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (11.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (77.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install langchain chromadb sentence-transformers pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1f9c9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13361bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Global variables initialized\n"
     ]
    }
   ],
   "source": [
    "embedding_model = None\n",
    "chroma_client = None\n",
    "collection = None\n",
    "\n",
    "print(\"‚úÖ Global variables initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be77b3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Loading embedding model (BAAI/bge-base-en-v1.5)...\n",
      "   This may take a moment on first run...\n",
      "‚úÖ Embedding model loaded successfully!\n",
      "üíæ Initializing ChromaDB...\n",
      "‚úÖ Persistent ChromaDB initialized with collection 'rag-chunks'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Collection(name=rag-chunks)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\"Load the sentence transformer model for embeddings\"\"\"\n",
    "    global embedding_model\n",
    "    print(\"üß† Loading embedding model (BAAI/bge-base-en-v1.5)...\")\n",
    "    print(\"   This may take a moment on first run...\")\n",
    "\n",
    "    # CHANGE: Use a better embedding model\n",
    "    embedding_model = SentenceTransformer('BAAI/bge-base-en-v1.5')\n",
    "    # Alternative options:\n",
    "    # embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    # embedding_model = SentenceTransformer('intfloat/e5-base-v2')\n",
    "\n",
    "    print(\"‚úÖ Embedding model loaded successfully!\")\n",
    "    return embedding_model\n",
    "\n",
    "\n",
    "def initialize_database(db_path=\"./vector_db_july200\"):\n",
    "    \"\"\"Initialize ChromaDB and create/get collection\"\"\"\n",
    "    global chroma_client, collection\n",
    "\n",
    "    print(\"üíæ Initializing ChromaDB...\")\n",
    "\n",
    "    # ‚úÖ FIX: Use a shared persistent collection\n",
    "    chroma_client = chromadb.PersistentClient(path=\"chromadb_store\")\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=\"rag-chunks\",\n",
    "        metadata={\"hnsw:space\": \"cosine\"}  # Better for semantic similarity\n",
    "    )\n",
    "    print(\"‚úÖ Persistent ChromaDB initialized with collection 'rag-chunks'\")\n",
    "\n",
    "    return collection\n",
    "\n",
    "\n",
    "# Initialize everything\n",
    "load_embedding_model()\n",
    "initialize_database()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4b88f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generic smart extraction functions loaded!\n"
     ]
    }
   ],
   "source": [
    "def extract_text_generic_smart(pdf_path, skip_first_pages=None):\n",
    "    \"\"\"Generic PDF extraction with smart page detection\"\"\"\n",
    "    print(f\"üìÑ Processing: {os.path.basename(pdf_path)}\")\n",
    "    \n",
    "    doc = None\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        total_pages = doc.page_count\n",
    "        \n",
    "        # Use smart detection if not specified\n",
    "        if skip_first_pages is None:\n",
    "            skip_first_pages = 5  # Default fallback\n",
    "            \n",
    "            # Look for Chapter 1 or Introduction\n",
    "            for page_num in range(5, min(20, total_pages)):\n",
    "                page = doc[page_num]\n",
    "                text = page.get_text().lower()\n",
    "                \n",
    "                if re.search(r'(chapter\\s+1|^1\\.\\s*introduction|^1\\.1\\s+|^introduction$)', text, re.MULTILINE):\n",
    "                    skip_first_pages = page_num\n",
    "                    print(f\"‚úÖ Found content start marker at page {page_num + 1}\")\n",
    "                    break\n",
    "        \n",
    "        print(f\"üìñ Total pages: {total_pages}\")\n",
    "        print(f\"‚è≠Ô∏è  Skipping first {skip_first_pages} pages\")\n",
    "        \n",
    "        # Rest of your existing code stays the same...\n",
    "        start_page = skip_first_pages\n",
    "        pages_to_process = total_pages - start_page\n",
    "        \n",
    "        print(f\"üìë Processing pages {start_page + 1} to {total_pages} ({pages_to_process} pages)\")\n",
    "        \n",
    "        raw_text = \"\"\n",
    "        page_texts = []\n",
    "        structured_content = []\n",
    "        \n",
    "        # Extract text from each page with better structure preservation\n",
    "        for page_num in range(start_page, total_pages):\n",
    "            try:\n",
    "                page = doc[page_num]\n",
    "                \n",
    "                # Get text with formatting info\n",
    "                blocks = page.get_text(\"dict\")\n",
    "                page_structured_text = []\n",
    "                current_paragraph = []\n",
    "                \n",
    "                for block in blocks[\"blocks\"]:\n",
    "                    if \"lines\" in block:\n",
    "                        for line in block[\"lines\"]:\n",
    "                            if line[\"spans\"]:\n",
    "                                text = \" \".join([span[\"text\"] for span in line[\"spans\"]])\n",
    "                                font_size = line[\"spans\"][0][\"size\"] if line[\"spans\"] else 12\n",
    "                                \n",
    "                                # If we hit a heading or empty line, save current paragraph\n",
    "                                if (font_size > 14 or not text.strip()) and current_paragraph:\n",
    "                                    combined_text = \" \".join(current_paragraph)\n",
    "                                    if combined_text.strip():\n",
    "                                        page_structured_text.append({\n",
    "                                            \"text\": combined_text.strip(),\n",
    "                                            \"is_heading\": False,\n",
    "                                            \"font_size\": 12,\n",
    "                                            \"page\": page_num + 1\n",
    "                                        })\n",
    "                                    current_paragraph = []\n",
    "                                \n",
    "                                # Add heading or continue paragraph\n",
    "                                if font_size > 14:\n",
    "                                    page_structured_text.append({\n",
    "                                        \"text\": text.strip(),\n",
    "                                        \"is_heading\": True,\n",
    "                                        \"font_size\": font_size,\n",
    "                                        \"page\": page_num + 1\n",
    "                                    })\n",
    "                                elif text.strip():\n",
    "                                    current_paragraph.append(text.strip())\n",
    "                \n",
    "                # Don't forget last paragraph\n",
    "                if current_paragraph:\n",
    "                    combined_text = \" \".join(current_paragraph)\n",
    "                    if combined_text.strip():\n",
    "                        page_structured_text.append({\n",
    "                            \"text\": combined_text.strip(),\n",
    "                            \"is_heading\": False,\n",
    "                            \"font_size\": 12,\n",
    "                            \"page\": page_num + 1\n",
    "                        })\n",
    "                \n",
    "                # Get plain text for compatibility\n",
    "                page_text = page.get_text()\n",
    "                page_texts.append(page_text)\n",
    "                structured_content.extend(page_structured_text)\n",
    "                raw_text += page_text + '\\n'\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not read page {page_num + 1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not raw_text.strip():\n",
    "            print(\"‚ùå No text found\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"üìù Extracted text from {len(page_texts)} pages\")\n",
    "        print(f\"üìè Total text length: {len(raw_text)} characters\")\n",
    "        \n",
    "        # Remove headers and footers\n",
    "        cleaned_text = remove_headers_footers_generic(page_texts)\n",
    "        \n",
    "        # Store structured content\n",
    "        doc.structured_content = structured_content\n",
    "        \n",
    "        return cleaned_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        if doc:\n",
    "            try:\n",
    "                doc.close()\n",
    "            except:\n",
    "                pass\n",
    "# Keep the rest of the functions in this cell as they are\n",
    "def remove_headers_footers_generic(page_texts):\n",
    "    \"\"\"Remove headers and footers based on position and repetition\"\"\"\n",
    "    print(\"üßπ Removing headers and footers generically...\")\n",
    "    \n",
    "    if not page_texts:\n",
    "        return \"\"\n",
    "    \n",
    "    # Split each page into lines\n",
    "    all_page_lines = []\n",
    "    for page_text in page_texts:\n",
    "        lines = [line.strip() for line in page_text.split('\\n') if line.strip()]\n",
    "        all_page_lines.append(lines)\n",
    "    \n",
    "    # Find common headers (first few lines that repeat across pages)\n",
    "    header_lines = find_common_position_lines(all_page_lines, position='top', max_lines=3)\n",
    "    \n",
    "    # Find common footers (last few lines that repeat across pages)\n",
    "    footer_lines = find_common_position_lines(all_page_lines, position='bottom', max_lines=3)\n",
    "    \n",
    "    print(f\"   Found {len(header_lines)} common header patterns\")\n",
    "    print(f\"   Found {len(footer_lines)} common footer patterns\")\n",
    "    \n",
    "    # Remove headers and footers from all pages\n",
    "    cleaned_pages = []\n",
    "    for page_lines in all_page_lines:\n",
    "        # Remove headers (from top)\n",
    "        start_idx = 0\n",
    "        for line in page_lines[:5]:  # Check first 5 lines\n",
    "            if line in header_lines:\n",
    "                start_idx += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Remove footers (from bottom)\n",
    "        end_idx = len(page_lines)\n",
    "        for line in reversed(page_lines[-5:]):  # Check last 5 lines\n",
    "            if line in footer_lines:\n",
    "                end_idx -= 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Keep the middle content\n",
    "        clean_lines = page_lines[start_idx:end_idx]\n",
    "        if clean_lines:\n",
    "            cleaned_pages.append('\\n'.join(clean_lines))\n",
    "    \n",
    "    final_text = '\\n\\n'.join(cleaned_pages)\n",
    "    \n",
    "    # Basic cleanup\n",
    "    final_text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', final_text)\n",
    "    final_text = re.sub(r'[ \\t]+', ' ', final_text)\n",
    "    \n",
    "    return final_text.strip()\n",
    "\n",
    "def find_common_position_lines(all_page_lines, position='top', max_lines=3):\n",
    "    \"\"\"Find lines that commonly appear at top or bottom of pages\"\"\"\n",
    "    if len(all_page_lines) < 2:\n",
    "        return set()\n",
    "    \n",
    "    # Count how often each line appears in the same position\n",
    "    line_counts = {}\n",
    "    \n",
    "    for page_lines in all_page_lines:\n",
    "        if not page_lines:\n",
    "            continue\n",
    "            \n",
    "        # Get lines from specified position\n",
    "        if position == 'top':\n",
    "            check_lines = page_lines[:max_lines]\n",
    "        else:  # bottom\n",
    "            check_lines = page_lines[-max_lines:]\n",
    "        \n",
    "        for line in check_lines:\n",
    "            if len(line) > 5:  # Ignore very short lines\n",
    "                line_counts[line] = line_counts.get(line, 0) + 1\n",
    "    \n",
    "    # Find lines that appear in at least 50% of pages\n",
    "    total_pages = len(all_page_lines)\n",
    "    threshold = max(2, total_pages * 0.5)\n",
    "    \n",
    "    common_lines = set()\n",
    "    for line, count in line_counts.items():\n",
    "        if count >= threshold:\n",
    "            common_lines.add(line)\n",
    "    \n",
    "    return common_lines\n",
    "\n",
    "print(\"‚úÖ Generic smart extraction functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6bb5e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PDF extraction function updated to use smart approach!\n"
     ]
    }
   ],
   "source": [
    "def extract_text_adaptive(pdf_path):\n",
    "    \"\"\"Use the new smart generic approach with automatic page detection\"\"\"\n",
    "    return extract_text_generic_smart(pdf_path)  # Will auto-detect start page\n",
    "\n",
    "print(\"‚úÖ PDF extraction function updated to use smart approach!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f1b1db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced chunking with better size guarantees loaded!\n"
     ]
    }
   ],
   "source": [
    "def evaluate_chunk_quality(chunk):\n",
    "    \"\"\"Less aggressive chunk quality evaluation\"\"\"\n",
    "    if len(chunk) < 100:\n",
    "        return 0\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    # 1. Length score - prefer longer chunks\n",
    "    length_score = min(len(chunk) / 1500, 1.0)\n",
    "    score += length_score * 0.4\n",
    "    \n",
    "    # 2. Word count\n",
    "    words = chunk.split()\n",
    "    if len(words) > 50:\n",
    "        score += 0.3\n",
    "    \n",
    "    # 3. Sentence structure\n",
    "    sentences = re.split(r'[.!?]+', chunk)\n",
    "    complete_sentences = [s for s in sentences if len(s.strip()) > 10]\n",
    "    if len(complete_sentences) >= 2:\n",
    "        score += 0.2\n",
    "    \n",
    "    # 4. Alphabetic content ratio\n",
    "    alpha_ratio = sum(c.isalpha() or c.isspace() for c in chunk) / len(chunk)\n",
    "    score += alpha_ratio * 0.1\n",
    "    \n",
    "    return score\n",
    "\n",
    "def quality_based_chunking(text, chunk_size=2000, chunk_overlap=500, min_quality=0.3):\n",
    "    \"\"\"Create LARGER chunks with better size guarantees\"\"\"\n",
    "    print(f\"üîÄ Creating quality chunks (size: {chunk_size})...\")\n",
    "    \n",
    "    # Use standard chunking with good separators\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\\n\", \"\\n\\n\", \".\\n\\n\", \".\\n\", \". \", \"\\n\", \" \"],\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    initial_chunks = splitter.split_text(text)\n",
    "    \n",
    "    # Combine small consecutive chunks\n",
    "    combined_chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for chunk in initial_chunks:\n",
    "        # If current chunk + new chunk is still under 1.5x chunk_size, combine them\n",
    "        if len(current_chunk) + len(chunk) < chunk_size * 1.5:\n",
    "            if current_chunk:\n",
    "                current_chunk += \"\\n\\n\" + chunk\n",
    "            else:\n",
    "                current_chunk = chunk\n",
    "        else:\n",
    "            # Save current chunk if it's big enough\n",
    "            if current_chunk and len(current_chunk) > 500:\n",
    "                combined_chunks.append(current_chunk)\n",
    "            current_chunk = chunk\n",
    "    \n",
    "    # Don't forget the last chunk\n",
    "    if current_chunk and len(current_chunk) > 500:\n",
    "        combined_chunks.append(current_chunk)\n",
    "    \n",
    "    print(f\"   Combined {len(initial_chunks)} initial chunks into {len(combined_chunks)} chunks\")\n",
    "    \n",
    "    # Filter by quality but be less aggressive\n",
    "    quality_chunks = []\n",
    "    low_quality_chunks = []\n",
    "    quality_scores = []\n",
    "    \n",
    "    for chunk in combined_chunks:\n",
    "        chunk = chunk.strip()\n",
    "        quality = evaluate_chunk_quality(chunk)\n",
    "        \n",
    "        if quality >= min_quality or len(chunk) > 1000:  # Keep chunks > 1000 chars regardless\n",
    "            quality_chunks.append(chunk)\n",
    "            quality_scores.append(quality)\n",
    "        else:\n",
    "            low_quality_chunks.append(chunk)\n",
    "    \n",
    "    # Try to salvage low quality chunks by combining them\n",
    "    if low_quality_chunks:\n",
    "        combined_low = \" \".join(low_quality_chunks)\n",
    "        if len(combined_low) > 1000:\n",
    "            quality_chunks.append(combined_low)\n",
    "            quality_scores.append(0.5)\n",
    "    \n",
    "    avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(quality_chunks)} quality chunks\")\n",
    "    print(f\"   Average quality score: {avg_quality:.2f}\")\n",
    "    if quality_chunks:\n",
    "        avg_len = sum(len(c) for c in quality_chunks) / len(quality_chunks)\n",
    "        print(f\"   Average chunk length: {avg_len:.0f} chars\")\n",
    "        print(f\"   Min chunk length: {min(len(c) for c in quality_chunks)} chars\")\n",
    "        print(f\"   Max chunk length: {max(len(c) for c in quality_chunks)} chars\")\n",
    "    \n",
    "    return quality_chunks\n",
    "\n",
    "# Remove the semantic_chunking function - we don't need it anymore\n",
    "\n",
    "print(\"‚úÖ Enhanced chunking with better size guarantees loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4e9c226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced embedding and storage functions loaded!\n"
     ]
    }
   ],
   "source": [
    "def generate_embeddings(chunks):\n",
    "    \"\"\"Generate embeddings for text chunks\"\"\"\n",
    "    print(f\"üß† Generating embeddings for {len(chunks)} chunks...\")\n",
    "    \n",
    "    # CHANGE: Add normalize_embeddings parameter\n",
    "    embeddings = embedding_model.encode(\n",
    "        chunks, \n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True  # This ensures cosine similarity works better\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Generated embeddings with shape: {embeddings.shape}\")\n",
    "    return embeddings\n",
    "def extract_metadata_from_chunk(chunk, chunk_id, pdf_name):\n",
    "    \"\"\"Extract metadata from chunk content\"\"\"\n",
    "    metadata = {\n",
    "        \"source\": pdf_name,\n",
    "        \"chunk_id\": chunk_id,\n",
    "        \"chunk_length\": len(chunk),\n",
    "        \"word_count\": len(chunk.split())\n",
    "    }\n",
    "    \n",
    "    # Try to extract chapter info\n",
    "    chapter_match = re.search(r'CHAPTER\\s+(\\d+)|Chapter\\s+(\\d+)', chunk)\n",
    "    if chapter_match:\n",
    "        metadata[\"chapter\"] = chapter_match.group(1) or chapter_match.group(2)\n",
    "    \n",
    "    # Try to extract section info\n",
    "    section_match = re.search(r'(\\d+\\.\\d+)\\s+', chunk)\n",
    "    if section_match:\n",
    "        metadata[\"section\"] = section_match.group(1)\n",
    "    \n",
    "    # Extract potential title (first line if it's short and looks like a heading)\n",
    "    first_line = chunk.split('\\n')[0].strip()\n",
    "    if len(first_line) < 100 and first_line.isupper():\n",
    "        metadata[\"potential_title\"] = first_line\n",
    "    \n",
    "    # Add preview\n",
    "    metadata[\"preview\"] = chunk[:200].replace('\\n', ' ')\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def store_chunks_in_db(chunks, embeddings, pdf_name):\n",
    "    \"\"\"Store chunks and embeddings in the vector database with rich metadata\"\"\"\n",
    "    print(f\"üíæ Storing {len(chunks)} chunks in vector database...\")\n",
    "    \n",
    "    # Prepare data for storage\n",
    "    ids = [f\"{pdf_name}_chunk_{i}\" for i in range(len(chunks))]\n",
    "    metadatas = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        metadata = extract_metadata_from_chunk(chunk, i, pdf_name)\n",
    "        metadata[\"total_chunks\"] = len(chunks)\n",
    "        metadata[\"position_ratio\"] = i / len(chunks)  # How far into doc\n",
    "        metadatas.append(metadata)\n",
    "    \n",
    "    # Add to collection\n",
    "    collection.add(\n",
    "        embeddings=embeddings.tolist(),\n",
    "        documents=chunks,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Successfully stored {len(chunks)} chunks from {pdf_name}\")\n",
    "    print(f\"   Chapters found: {len(set(m.get('chapter', 'none') for m in metadatas))}\")\n",
    "    print(f\"   Sections found: {len(set(m.get('section', 'none') for m in metadatas))}\")\n",
    "    \n",
    "    return len(chunks)\n",
    "\n",
    "print(\"‚úÖ Enhanced embedding and storage functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11501bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete processing pipeline loaded!\n"
     ]
    }
   ],
   "source": [
    "def process_single_pdf(pdf_path):\n",
    "    \"\"\"Process a single PDF file with duplicate checking\"\"\"\n",
    "    pdf_name = os.path.basename(pdf_path)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÄ Processing: {pdf_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Check if already processed\n",
    "    try:\n",
    "        existing = collection.get(where={\"source\": pdf_name})\n",
    "        if existing['ids']:\n",
    "            print(f\"‚ö†Ô∏è  {pdf_name} already in database with {len(existing['ids'])} chunks\")\n",
    "            response = input(\"Re-process? (yes/no): \")\n",
    "            if response.lower() != 'yes':\n",
    "                return 0\n",
    "            else:\n",
    "                # Delete existing chunks\n",
    "                collection.delete(ids=existing['ids'])\n",
    "                print(f\"üóëÔ∏è  Deleted {len(existing['ids'])} existing chunks\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Extract and clean text\n",
    "        text = extract_text_adaptive(pdf_path)\n",
    "        if not text:\n",
    "            print(f\"‚ùå Failed to extract meaningful text from {pdf_name}\")\n",
    "            return 0\n",
    "        \n",
    "        print(f\"üìè Extracted text length: {len(text)} characters\")\n",
    "        \n",
    "        # Step 2: Quality-based chunking with new parameters\n",
    "        chunks = quality_based_chunking(text, chunk_size=2000, chunk_overlap=500, min_quality=0.3)\n",
    "        if not chunks:\n",
    "            print(f\"‚ùå No quality chunks created from {pdf_name}\")\n",
    "            return 0\n",
    "        \n",
    "        # Step 3: Generate embeddings\n",
    "        embeddings = generate_embeddings(chunks)\n",
    "        \n",
    "        # Step 4: Store in database\n",
    "        chunk_count = store_chunks_in_db(chunks, embeddings, pdf_name)\n",
    "        \n",
    "        print(f\"üéâ Successfully processed {pdf_name}!\")\n",
    "        return chunk_count\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {pdf_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 0\n",
    "def list_available_pdfs(folder_path=\"./pdfs\"):\n",
    "    \"\"\"List all PDF files in the folder with details\"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"‚ùå Folder {folder_path} does not exist!\")\n",
    "        return []\n",
    "    \n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "    \n",
    "    print(f\"üìÅ PDFs found in {folder_path}:\")\n",
    "    if pdf_files:\n",
    "        for i, pdf in enumerate(pdf_files, 1):\n",
    "            \n",
    "            file_path = os.path.join(folder_path, pdf)\n",
    "            file_size = os.path.getsize(file_path) / (1024*1024)  # MB\n",
    "            print(f\"  {i}. {pdf} ({file_size:.1f} MB)\")\n",
    "    else:\n",
    "        print(\"  No PDF files found!\")\n",
    "        print(f\"  üëâ Please add your PDF files to the '{folder_path}' folder\")\n",
    "    \n",
    "    return pdf_files\n",
    "\n",
    "def process_all_pdfs(folder_path=\"./pdfs\"):\n",
    "    \"\"\"Process all PDF files in the folder with adaptive cleaning\"\"\"\n",
    "    pdf_files = list_available_pdfs(folder_path)\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(\"‚ùå No PDF files to process!\")\n",
    "        return [], 0\n",
    "    \n",
    "    print(f\"\\nüîÑ Starting to process {len(pdf_files)} PDF files...\")\n",
    "    print(\"Using adaptive cleaning and quality-based chunking...\")\n",
    "    \n",
    "    total_chunks = 0\n",
    "    processed_files = []\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(folder_path, pdf_file)\n",
    "        chunk_count = process_single_pdf(pdf_path)\n",
    "        \n",
    "        if chunk_count > 0:\n",
    "            processed_files.append((pdf_file, chunk_count))\n",
    "            total_chunks += chunk_count\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üéâ PROCESSING COMPLETE!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üìä Files processed: {len(processed_files)}/{len(pdf_files)}\")\n",
    "    print(f\"üìà Total chunks stored: {total_chunks}\")\n",
    "    print(\"\\nüìã Summary:\")\n",
    "    for filename, chunk_count in processed_files:\n",
    "        print(f\"  ‚úÖ {filename}: {chunk_count} chunks\")\n",
    "    \n",
    "    return processed_files, total_chunks\n",
    "\n",
    "print(\"‚úÖ Complete processing pipeline loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84428b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced search and utility functions loaded!\n"
     ]
    }
   ],
   "source": [
    "eranker = None\n",
    "\n",
    "def initialize_reranker():\n",
    "    \"\"\"Initialize the cross-encoder for reranking\"\"\"\n",
    "    global reranker\n",
    "    print(\"üéØ Loading reranking model...\")\n",
    "    reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    print(\"‚úÖ Reranker loaded!\")\n",
    "    return reranker# Add this import at the top of the cell\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Initialize reranker globally\n",
    "r\n",
    "\n",
    "def search_similar_chunks(query, n_results=5, filter_dict=None, min_relevance=0.5):\n",
    "    \"\"\"Enhanced search with reranking and relevance filtering\"\"\"\n",
    "    print(f\"üîç Searching for: '{query}'\")\n",
    "    \n",
    "    try:\n",
    "        # Generate embedding for query with normalization\n",
    "        query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n",
    "        \n",
    "        # Get more candidates for reranking (3x the requested amount)\n",
    "        search_params = {\n",
    "            \"query_embeddings\": query_embedding.tolist(),\n",
    "            \"n_results\": min(n_results * 4, 40)  # Get up to 40 candidates\n",
    "        }\n",
    "        \n",
    "        if filter_dict:\n",
    "            search_params[\"where\"] = filter_dict\n",
    "            print(f\"   With filters: {filter_dict}\")\n",
    "        \n",
    "        # Search in collection\n",
    "        results = collection.query(**search_params)\n",
    "        \n",
    "        if not results['documents'][0]:\n",
    "            print(\"‚ùå No results found\")\n",
    "            return []\n",
    "        \n",
    "        # Prepare candidates for reranking\n",
    "        candidates = []\n",
    "        for i, doc in enumerate(results['documents'][0]):\n",
    "            candidates.append({\n",
    "                'document': doc,\n",
    "                'metadata': results['metadatas'][0][i],\n",
    "                'initial_score': 1 - results['distances'][0][i],\n",
    "                'id': results['ids'][0][i]\n",
    "            })\n",
    "        \n",
    "        # Initialize reranker if not already done\n",
    "        global reranker\n",
    "        if reranker is None:\n",
    "            initialize_reranker()\n",
    "        \n",
    "        # Rerank using cross-encoder\n",
    "        print(f\"üéØ Reranking {len(candidates)} candidates...\")\n",
    "        pairs = [[query, c['document'][:512]] for c in candidates]  # Limit text for reranker\n",
    "        rerank_scores = reranker.predict(pairs)\n",
    "        \n",
    "        # Combine scores and filter\n",
    "        final_results = []\n",
    "        for i, candidate in enumerate(candidates):\n",
    "            # Combine initial embedding score with rerank score\n",
    "            combined_score = (candidate['initial_score'] * 0.3) + (rerank_scores[i] * 0.7)\n",
    "            \n",
    "            if combined_score >= min_relevance:\n",
    "                candidate['rerank_score'] = rerank_scores[i]\n",
    "                candidate['final_score'] = combined_score\n",
    "                final_results.append(candidate)\n",
    "        \n",
    "        # Sort by final score\n",
    "        final_results.sort(key=lambda x: x['final_score'], reverse=True)\n",
    "        top_results = final_results[:n_results]\n",
    "        \n",
    "        if not top_results:\n",
    "            print(f\"‚ö†Ô∏è No results above relevance threshold {min_relevance}\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"üìã Found {len(candidates)} candidates, {len(final_results)} above threshold, returning top {len(top_results)}:\")\n",
    "        \n",
    "        for i, result in enumerate(top_results):\n",
    "            metadata = result['metadata']\n",
    "            print(f\"\\n{i+1}. üìÑ {metadata['source']} (ID: {result['id'][-8:]})\") \n",
    "            if metadata.get('chapter'):\n",
    "                print(f\"   üìñ Chapter {metadata['chapter']}\")\n",
    "            if metadata.get('section'):\n",
    "                print(f\"   üìë Section {metadata['section']}\")\n",
    "            print(f\"   üìä Initial: {result['initial_score']:.3f}, Rerank: {result['rerank_score']:.3f}, Final: {result['final_score']:.3f}\")\n",
    "            print(f\"   üìè Chunk size: {len(result['document'])} chars\")\n",
    "            print(f\"   üìù Preview: {result['document'][:200]}...\")\n",
    "        \n",
    "        return top_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "# Update build_context_for_chatbot to handle no results better\n",
    "def build_context_for_chatbot(query, n_chunks=3, min_relevance=0.5):\n",
    "    \"\"\"Build context for chatbot from search results\"\"\"\n",
    "    results = search_similar_chunks(query, n_chunks, min_relevance=min_relevance)\n",
    "    \n",
    "    if not results:\n",
    "        return \"I couldn't find sufficiently relevant information in the documents to answer your question. Please try rephrasing your query or ask about topics covered in the uploaded documents.\"\n",
    "    \n",
    "    # Only use results with high confidence\n",
    "    high_confidence_results = [r for r in results if r['final_score'] >= 0.7]\n",
    "    \n",
    "    if high_confidence_results:\n",
    "        context = \"Based on the following highly relevant information from the documents:\\n\\n\"\n",
    "    else:\n",
    "        context = \"Based on the following potentially relevant information from the documents (moderate confidence):\\n\\n\"\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        metadata = result['metadata']\n",
    "        context += f\"[Source {i+1}: {metadata['source']}\"\n",
    "        \n",
    "        if metadata.get('chapter'):\n",
    "            context += f\", Chapter {metadata['chapter']}\"\n",
    "        if metadata.get('section'):\n",
    "            context += f\", Section {metadata['section']}\"\n",
    "        \n",
    "        context += f\" - Relevance: {result['final_score']:.2f}]\\n\"\n",
    "        context += f\"{result['document']}\\n\\n\"\n",
    "        context += \"-\" * 50 + \"\\n\\n\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "def get_database_stats():\n",
    "    \"\"\"Get comprehensive database statistics\"\"\"\n",
    "    try:\n",
    "        count = collection.count()\n",
    "        print(f\"üìä Database Statistics:\")\n",
    "        print(f\"  üíæ Total chunks stored: {count}\")\n",
    "        \n",
    "        if count > 0:\n",
    "            # Get sample to analyze\n",
    "            sample = collection.get(limit=min(count, 1000))\n",
    "            \n",
    "            # Count chunks per document\n",
    "            source_counts = {}\n",
    "            chapter_counts = {}\n",
    "            \n",
    "            for meta in sample['metadatas']:\n",
    "                source = meta['source']\n",
    "                source_counts[source] = source_counts.get(source, 0) + 1\n",
    "                \n",
    "                if meta.get('chapter'):\n",
    "                    chapter_key = f\"{source} - Chapter {meta['chapter']}\"\n",
    "                    chapter_counts[chapter_key] = chapter_counts.get(chapter_key, 0) + 1\n",
    "            \n",
    "            print(f\"  üìö Documents stored: {len(source_counts)}\")\n",
    "            print(f\"  üìñ Chapters identified: {len(chapter_counts)}\")\n",
    "            print(f\"\\n  üìà Chunks per document:\")\n",
    "            for source, chunk_count in source_counts.items():\n",
    "                print(f\"     ‚Ä¢ {source}: {chunk_count} chunks\")\n",
    "        \n",
    "        return count\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting database stats: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "def show_sample_chunks(n_samples=3):\n",
    "    \"\"\"Show sample chunks from the database\"\"\"\n",
    "    try:\n",
    "        sample = collection.peek(limit=n_samples)\n",
    "        print(f\"üìã Sample chunks (showing {n_samples}):\")\n",
    "        \n",
    "        for i, doc in enumerate(sample['documents']):\n",
    "            metadata = sample['metadatas'][i]\n",
    "            print(f\"\\n{i+1}. üìÑ {metadata['source']} (Chunk {metadata['chunk_id']})\")\n",
    "            if metadata.get('chapter'):\n",
    "                print(f\"   üìñ Chapter {metadata['chapter']}\")\n",
    "            if metadata.get('section'):\n",
    "                print(f\"   üìë Section {metadata['section']}\")\n",
    "            print(f\"   üìù Content: {doc[:200]}...\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error showing samples: {e}\")\n",
    "\n",
    "def answer_question(question, n_chunks=3, verbose=False):\n",
    "    \"\"\"\n",
    "    Simple Q&A function for chatbot integration\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"ü§î Question: {question}\")\n",
    "    \n",
    "    # Get context\n",
    "    context = build_context_for_chatbot(question, n_chunks)\n",
    "    \n",
    "    # Format for LLM\n",
    "    prompt = f\"\"\"Based on the following context from technical documents, please answer the question.\n",
    "    \n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüìù Context has {len(context.split())} words\")\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Keep the other utility functions as they are\n",
    "\n",
    "print(\"‚úÖ Enhanced search and utility functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bc94744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\rag\\999\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed3aadb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Folder C:\\Users\\rohan\\Desktop\\999\\pdfs does not exist!\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "pdf_folder = r\"C:\\Users\\rohan\\Desktop\\999\\pdfs\"\n",
    "print(list_available_pdfs(pdf_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec5191dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now in: c:\\rag\\999\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Change working directory to the notebook location\n",
    "os.chdir(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "print(\"Now in:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8e2773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ PDFs found in ./pdfs:\n",
      "  1. 7_Report - khushi kiran.pdf (2.9 MB)\n",
      "  2. 9_Report - Sragvi Shetty.pdf (2.5 MB)\n",
      "  3. bs.pdf (0.8 MB)\n",
      "  4. s1.pdf (1.9 MB)\n",
      "  5. some.pdf (7.5 MB)\n",
      "PDFs: ['7_Report - khushi kiran.pdf', '9_Report - Sragvi Shetty.pdf', 'bs.pdf', 's1.pdf', 'some.pdf']\n"
     ]
    }
   ],
   "source": [
    "print(\"PDFs:\", list_available_pdfs(\"./pdfs\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6262399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Checking available PDFs...\n",
      "üìÅ PDFs found in ./pdfs:\n",
      "  1. 7_Report - khushi kiran.pdf (2.9 MB)\n",
      "  2. 9_Report - Sragvi Shetty.pdf (2.5 MB)\n",
      "  3. bs.pdf (0.8 MB)\n",
      "  4. s1.pdf (1.9 MB)\n",
      "  5. some.pdf (7.5 MB)\n",
      "\n",
      "üöÄ Starting processing of 5 PDF files...\n",
      "üìÅ PDFs found in ./pdfs:\n",
      "  1. 7_Report - khushi kiran.pdf (2.9 MB)\n",
      "  2. 9_Report - Sragvi Shetty.pdf (2.5 MB)\n",
      "  3. bs.pdf (0.8 MB)\n",
      "  4. s1.pdf (1.9 MB)\n",
      "  5. some.pdf (7.5 MB)\n",
      "\n",
      "üîÑ Starting to process 5 PDF files...\n",
      "Using adaptive cleaning and quality-based chunking...\n",
      "\n",
      "============================================================\n",
      "üöÄ Processing: 7_Report - khushi kiran.pdf\n",
      "============================================================\n",
      "‚ö†Ô∏è  7_Report - khushi kiran.pdf already in database with 22 chunks\n",
      "\n",
      "============================================================\n",
      "üöÄ Processing: 9_Report - Sragvi Shetty.pdf\n",
      "============================================================\n",
      "‚ö†Ô∏è  9_Report - Sragvi Shetty.pdf already in database with 20 chunks\n",
      "\n",
      "============================================================\n",
      "üöÄ Processing: bs.pdf\n",
      "============================================================\n",
      "‚ö†Ô∏è  bs.pdf already in database with 25 chunks\n",
      "\n",
      "============================================================\n",
      "üöÄ Processing: s1.pdf\n",
      "============================================================\n",
      "‚ö†Ô∏è  s1.pdf already in database with 22 chunks\n",
      "\n",
      "============================================================\n",
      "üöÄ Processing: some.pdf\n",
      "============================================================\n",
      "‚ö†Ô∏è  some.pdf already in database with 33 chunks\n",
      "\n",
      "======================================================================\n",
      "üéâ PROCESSING COMPLETE!\n",
      "======================================================================\n",
      "üìä Files processed: 0/5\n",
      "üìà Total chunks stored: 0\n",
      "\n",
      "üìã Summary:\n"
     ]
    }
   ],
   "source": [
    "## Cell 10: Execute Processing\n",
    "\n",
    "# Check what PDFs are available\n",
    "print(\"üìÅ Checking available PDFs...\")\n",
    "pdf_files = list_available_pdfs(\"./pdfs\")\n",
    "\n",
    "# Process all PDFs if any are found\n",
    "if pdf_files:\n",
    "    print(f\"\\nüöÄ Starting processing of {len(pdf_files)} PDF files...\")\n",
    "    processed_files, total_chunks = process_all_pdfs(\"./pdfs\")\n",
    "else:\n",
    "    print(\"\\nüìù To get started:\")\n",
    "    print(\"1. Add your PDF files to the './pdfs' folder\")\n",
    "    print(\"2. Rerun this cell to process them\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f1ffbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing enhanced features:\n",
      "\n",
      "üìö Testing chapter-specific search:\n",
      "üîç Searching for: 'methodology'\n",
      "   With filters: {'chapter': '3'}\n",
      "üéØ Loading reranking model...\n",
      "‚úÖ Reranker loaded!\n",
      "üéØ Reranking 1 candidates...\n",
      "‚ö†Ô∏è No results above relevance threshold 0.5\n",
      "\n",
      "ü§ñ Testing chatbot context building:\n",
      "üîç Searching for: 'plant disease detection using GAN'\n",
      "üéØ Reranking 12 candidates...\n",
      "üìã Found 12 candidates, 9 above threshold, returning top 3:\n",
      "\n",
      "1. üìÑ bs.pdf (ID: chunk_23)\n",
      "   üìä Initial: 0.807, Rerank: 7.389, Final: 5.414\n",
      "   üìè Chunk size: 1509 chars\n",
      "   üìù Preview: 44 of 46\n",
      "REFERENCES/BIBLIOGRAPHY\n",
      "[1] P. Harinadha and C. Krishna Mohan, \"Leaf Based Tomato Plant Disease Detection Using\n",
      "Generated Images from WGP-ESR GAN,\" 2023 International Conference on Data Scien...\n",
      "\n",
      "2. üìÑ bs.pdf (ID: chunk_20)\n",
      "   üìë Section 6.4\n",
      "   üìä Initial: 0.826, Rerank: 7.176, Final: 5.271\n",
      "   üìè Chunk size: 2981 chars\n",
      "   üìù Preview: 35 of 46\n",
      "We aim to have an automated plant disease detection model while maintaining the accuracy of the\n",
      "prediction. The integration of GAN, the classification model, and the clustering model increase...\n",
      "\n",
      "3. üìÑ bs.pdf (ID: chunk_15)\n",
      "   üìë Section 2.2\n",
      "   üìä Initial: 0.802, Rerank: 5.019, Final: 3.754\n",
      "   üìè Chunk size: 1925 chars\n",
      "   üìù Preview: 29 of 46\n",
      "4) Scaling and Adaptability: We should be able to allow users to upload single or multiple\n",
      "photos for classification. We should also be able to show that the models will be finetuned in\n",
      "the f...\n",
      "Context preview:\n",
      "Based on the following highly relevant information from the documents:\n",
      "\n",
      "[Source 1: bs.pdf - Relevance: 5.41]\n",
      "44 of 46\n",
      "REFERENCES/BIBLIOGRAPHY\n",
      "[1] P. Harinadha and C. Krishna Mohan, \"Leaf Based Tomato Plant Disease Detection Using\n",
      "Generated Images from WGP-ESR GAN,\" 2023 International Conference on Data Science and\n",
      "Network\n",
      "Security\n",
      "(ICDSNS),\n",
      "Tiptur,\n",
      "India,\n",
      "2023,\n",
      "pp.\n",
      "01-06,\n",
      "doi:\n",
      "10.1109/ICDSNS58469.2023.10245332.\n",
      "[2] Y. Zhao et al., \"Plant Disease Detection Using Generated Leaves Based on DoubleGA...\n",
      "\n",
      "‚úÖ Enhanced RAG system ready for chatbot integration!\n"
     ]
    }
   ],
   "source": [
    "# Your existing tests...\n",
    "\n",
    "# Add these new tests\n",
    "print(\"\\nüß™ Testing enhanced features:\")\n",
    "\n",
    "# Test filtered search\n",
    "print(\"\\nüìö Testing chapter-specific search:\")\n",
    "chapter_search = search_similar_chunks(\n",
    "    \"methodology\", \n",
    "    n_results=2,\n",
    "    filter_dict={\"chapter\": \"3\"}  # Search only in chapter 3\n",
    ")\n",
    "\n",
    "# Test context building\n",
    "print(\"\\nü§ñ Testing chatbot context building:\")\n",
    "context = build_context_for_chatbot(\"plant disease detection using GAN\", n_chunks=3)\n",
    "print(\"Context preview:\")\n",
    "print(context[:500] + \"...\")\n",
    "\n",
    "print(\"\\n‚úÖ Enhanced RAG system ready for chatbot integration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38624a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running comprehensive tests...\n",
      "\n",
      "üìä Database Overview:\n",
      "üìä Database Statistics:\n",
      "  üíæ Total chunks stored: 122\n",
      "  üìö Documents stored: 5\n",
      "  üìñ Chapters identified: 16\n",
      "\n",
      "  üìà Chunks per document:\n",
      "     ‚Ä¢ 7_Report - khushi kiran.pdf: 22 chunks\n",
      "     ‚Ä¢ 9_Report - Sragvi Shetty.pdf: 20 chunks\n",
      "     ‚Ä¢ bs.pdf: 25 chunks\n",
      "     ‚Ä¢ s1.pdf: 22 chunks\n",
      "     ‚Ä¢ some.pdf: 33 chunks\n",
      "\n",
      "üìÑ Sample stored chunks:\n",
      "üìã Sample chunks (showing 3):\n",
      "\n",
      "1. üìÑ 7_Report - khushi kiran.pdf (Chunk 0)\n",
      "   üìë Section 3.1\n",
      "   üìù Content: TABLE OF CONTENTS\n",
      "Chapter No.\n",
      "Title\n",
      "Page No.\n",
      "1.\n",
      "INTRODUCTION\n",
      "01\n",
      "2.\n",
      "PROBLEM DEFINITION\n",
      "02\n",
      "3.\n",
      "LITERATURE SURVEY\n",
      "03-18\n",
      "3.1 Celeb-DF: A Large-Scale Challenging Dataset for\n",
      "DeepFake Forensics\n",
      "03\n",
      "3.1.1 Abst...\n",
      "\n",
      "2. üìÑ 7_Report - khushi kiran.pdf (Chunk 1)\n",
      "   üìñ Chapter 1\n",
      "   üìë Section 2.1\n",
      "   üìù Content: LIST OF FIGURES\n",
      "Figure No.\n",
      "Title\n",
      "Page No.\n",
      "6.2.1\n",
      "Architecture diagram\n",
      "28\n",
      "6.2.2\n",
      "EAR stream\n",
      "29\n",
      "6.2.3\n",
      "Encoder\n",
      "29\n",
      "6.2.4\n",
      "Decoder\n",
      "30\n",
      "6.2.5\n",
      "Localization of Deepfaked regions and\n",
      "generate % for Extent of Deepf...\n",
      "\n",
      "3. üìÑ 7_Report - khushi kiran.pdf (Chunk 2)\n",
      "   üìù Content: Page No. 2\n",
      "Dept. of CSE\n",
      "Jan - May, 2024\n",
      "PROBLEM DEFINITION\n",
      "As technology develops, it becomes more difficult to distinguish between actual and\n",
      "manipulated photos due to the widespread occurrence of de...\n",
      "\n",
      "üîç Testing various search queries:\n",
      "\n",
      "==================================================\n",
      "Query: 'plant disease detection methodology'\n",
      "üîç Searching for: 'plant disease detection methodology'\n",
      "üéØ Reranking 8 candidates...\n",
      "üìã Found 8 candidates, 8 above threshold, returning top 2:\n",
      "\n",
      "1. üìÑ bs.pdf (ID: _chunk_2)\n",
      "   üìñ Chapter 2\n",
      "   üìë Section 3.1\n",
      "   üìä Initial: 0.792, Rerank: 7.580, Final: 5.544\n",
      "   üìè Chunk size: 2603 chars\n",
      "   üìù Preview: 12 of 46\n",
      "CHAPTER 2\n",
      "Problem Definition\n",
      "The existing manual methods for plant disease detection in agriculture are inefficient and prone to\n",
      "errors, leading to significant loss of crops and are a huge co...\n",
      "\n",
      "2. üìÑ bs.pdf (ID: chunk_10)\n",
      "   üìñ Chapter 5\n",
      "   üìë Section 5.1\n",
      "   üìä Initial: 0.775, Rerank: 6.623, Final: 4.869\n",
      "   üìè Chunk size: 1727 chars\n",
      "   üìù Preview: 23 of 46\n",
      "CHAPTER 5\n",
      "Systems Requirements Specification\n",
      "5.1 Product Perspective\n",
      "5.1.1 Product Features\n",
      "1) Automated Disease Detection: Our plant disease detection model automatically detects plant\n",
      "disea...\n",
      "\n",
      "==================================================\n",
      "Query: 'GAN architecture'\n",
      "üîç Searching for: 'GAN architecture'\n",
      "üéØ Reranking 8 candidates...\n",
      "üìã Found 8 candidates, 1 above threshold, returning top 1:\n",
      "\n",
      "1. üìÑ bs.pdf (ID: chunk_19)\n",
      "   üìë Section 3.1\n",
      "   üìä Initial: 0.758, Rerank: 4.035, Final: 3.052\n",
      "   üìè Chunk size: 2254 chars\n",
      "   üìù Preview: 34 of 46\n",
      "6.3.1 Design Description\n",
      "Our system mainly consists of four parts--preprocessing, the GAN Model, the Classification Model,\n",
      "and the clustering Model.\n",
      "In the first stage of the architecture, we...\n",
      "\n",
      "==================================================\n",
      "Query: 'machine learning classification'\n",
      "üîç Searching for: 'machine learning classification'\n",
      "üéØ Reranking 8 candidates...\n",
      "üìã Found 8 candidates, 1 above threshold, returning top 1:\n",
      "\n",
      "1. üìÑ bs.pdf (ID: chunk_15)\n",
      "   üìë Section 2.2\n",
      "   üìä Initial: 0.659, Rerank: 3.827, Final: 2.876\n",
      "   üìè Chunk size: 1925 chars\n",
      "   üìù Preview: 29 of 46\n",
      "4) Scaling and Adaptability: We should be able to allow users to upload single or multiple\n",
      "photos for classification. We should also be able to show that the models will be finetuned in\n",
      "the f...\n",
      "\n",
      "==================================================\n",
      "Query: 'results and evaluation'\n",
      "üîç Searching for: 'results and evaluation'\n",
      "üéØ Reranking 8 candidates...\n",
      "‚ö†Ô∏è No results above relevance threshold 0.5\n",
      "\n",
      "ü§ñ Testing chatbot context generation:\n",
      "\n",
      "Question: What is the methodology for plant disease detection?\n",
      "üîç Searching for: 'What is the methodology for plant disease detection?'\n",
      "üéØ Reranking 12 candidates...\n",
      "üìã Found 12 candidates, 11 above threshold, returning top 3:\n",
      "\n",
      "1. üìÑ bs.pdf (ID: _chunk_2)\n",
      "   üìñ Chapter 2\n",
      "   üìë Section 3.1\n",
      "   üìä Initial: 0.786, Rerank: 7.044, Final: 5.166\n",
      "   üìè Chunk size: 2603 chars\n",
      "   üìù Preview: 12 of 46\n",
      "CHAPTER 2\n",
      "Problem Definition\n",
      "The existing manual methods for plant disease detection in agriculture are inefficient and prone to\n",
      "errors, leading to significant loss of crops and are a huge co...\n",
      "\n",
      "2. üìÑ bs.pdf (ID: chunk_14)\n",
      "   üìñ Chapter 6\n",
      "   üìë Section 6.1\n",
      "   üìä Initial: 0.766, Rerank: 6.551, Final: 4.815\n",
      "   üìè Chunk size: 1723 chars\n",
      "   üìù Preview: 28 of 46\n",
      "CHAPTER 6\n",
      "System Design\n",
      "6.1 Current System\n",
      "The systems that exist for plant disease detection mainly follow a two-step process which is to identify\n",
      "if the plant is healthy or unhealthy using ...\n",
      "\n",
      "3. üìÑ bs.pdf (ID: chunk_10)\n",
      "   üìñ Chapter 5\n",
      "   üìë Section 5.1\n",
      "   üìä Initial: 0.758, Rerank: 5.228, Final: 3.887\n",
      "   üìè Chunk size: 1727 chars\n",
      "   üìù Preview: 23 of 46\n",
      "CHAPTER 5\n",
      "Systems Requirements Specification\n",
      "5.1 Product Perspective\n",
      "5.1.1 Product Features\n",
      "1) Automated Disease Detection: Our plant disease detection model automatically detects plant\n",
      "disea...\n",
      "Context length: 6470 characters\n",
      "First 300 chars of context:\n",
      "Based on the following highly relevant information from the documents:\n",
      "\n",
      "[Source 1: bs.pdf, Chapter 2, Section 3.1 - Relevance: 5.17]\n",
      "12 of 46\n",
      "CHAPTER 2\n",
      "Problem Definition\n",
      "The existing manual methods for plant disease detection in agriculture are inefficient and prone to\n",
      "errors, leading to significan...\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Comprehensive Testing\n",
    "print(\"üß™ Running comprehensive tests...\\n\")\n",
    "\n",
    "# 1. Database stats\n",
    "print(\"üìä Database Overview:\")\n",
    "get_database_stats()\n",
    "\n",
    "# 2. Sample chunks\n",
    "print(\"\\nüìÑ Sample stored chunks:\")\n",
    "show_sample_chunks(3)\n",
    "\n",
    "# 3. Test different search queries\n",
    "test_queries = [\n",
    "    \"plant disease detection methodology\",\n",
    "    \"GAN architecture\",\n",
    "    \"machine learning classification\",\n",
    "    \"results and evaluation\"\n",
    "]\n",
    "\n",
    "print(\"\\nüîç Testing various search queries:\")\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    results = search_similar_chunks(query, n_results=2)\n",
    "    \n",
    "# 4. Test chatbot context generation\n",
    "print(\"\\nü§ñ Testing chatbot context generation:\")\n",
    "sample_questions = [\n",
    "    \"What is the methodology for plant disease detection?\",\n",
    "    \"How does the GAN model work in this system?\",\n",
    "    \"What are the evaluation metrics used?\"\n",
    "]\n",
    "\n",
    "for question in sample_questions[:1]:  # Just test one for brevity\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    context = build_context_for_chatbot(question, n_chunks=3)\n",
    "    print(f\"Context length: {len(context)} characters\")\n",
    "    print(\"First 300 chars of context:\")\n",
    "    print(context[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c3d0ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (11.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jsdha\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "044135e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing current chunk sizes...\n",
      "\n",
      "üìè Chunk size statistics:\n",
      "   Min: 1230 chars\n",
      "   Max: 2961 chars\n",
      "   Average: 2087 chars\n",
      "   Total chunks: 122\n",
      "\n",
      "üìä Size distribution:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Chunk sizes look good!\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Analyze Current Chunk Sizes\n",
    "print(\"üîç Analyzing current chunk sizes...\\n\")\n",
    "\n",
    "# Get a sample of chunks\n",
    "sample = collection.get(limit=20)\n",
    "\n",
    "chunk_lengths = [len(doc) for doc in sample['documents']]\n",
    "print(f\"üìè Chunk size statistics:\")\n",
    "print(f\"   Min: {min(chunk_lengths)} chars\")\n",
    "print(f\"   Max: {max(chunk_lengths)} chars\")\n",
    "print(f\"   Average: {sum(chunk_lengths)//len(chunk_lengths)} chars\")\n",
    "print(f\"   Total chunks: {collection.count()}\")\n",
    "\n",
    "# Show distribution\n",
    "print(f\"\\nüìä Size distribution:\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist([len(doc) for doc in collection.get(limit=100)['documents']], bins=20)\n",
    "plt.xlabel('Chunk Size (characters)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Chunk Sizes')\n",
    "plt.show()\n",
    "\n",
    "# Check if chunks are too small\n",
    "small_chunks = [l for l in chunk_lengths if l < 500]\n",
    "if len(small_chunks) > len(chunk_lengths) * 0.3:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Many chunks are small! Consider reprocessing with larger chunk_size.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Chunk sizes look good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0207690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Operation cancelled\n"
     ]
    }
   ],
   "source": [
    "def clear_database():\n",
    "    \"\"\"Clear all data from the database\"\"\"\n",
    "    try:\n",
    "        count_before = collection.count()\n",
    "        if count_before == 0:\n",
    "            print(\"‚ÑπÔ∏è  Database is already empty\")\n",
    "            return\n",
    "            \n",
    "        response = input(f\"‚ö†Ô∏è  Delete all {count_before} chunks? Type 'YES' to confirm: \")\n",
    "        if response == 'YES':\n",
    "            # Get all IDs\n",
    "            all_ids = collection.get()['ids']\n",
    "            if all_ids:\n",
    "                collection.delete(ids=all_ids)\n",
    "                print(f\"‚úÖ Deleted {count_before} chunks from collection\")\n",
    "            \n",
    "            # Verify it's empty\n",
    "            count_after = collection.count()\n",
    "            print(f\"üìä Collection now has {count_after} chunks\")\n",
    "        else:\n",
    "            print(\"‚ùå Operation cancelled\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error clearing database: {e}\")\n",
    "\n",
    "# Test the function\n",
    "clear_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "948e2025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['7_Report - khushi kiran.pdf_chunk_0',\n",
       "  '7_Report - khushi kiran.pdf_chunk_1',\n",
       "  '7_Report - khushi kiran.pdf_chunk_2',\n",
       "  '7_Report - khushi kiran.pdf_chunk_3',\n",
       "  '7_Report - khushi kiran.pdf_chunk_4',\n",
       "  '7_Report - khushi kiran.pdf_chunk_5',\n",
       "  '7_Report - khushi kiran.pdf_chunk_6',\n",
       "  '7_Report - khushi kiran.pdf_chunk_7',\n",
       "  '7_Report - khushi kiran.pdf_chunk_8',\n",
       "  '7_Report - khushi kiran.pdf_chunk_9'],\n",
       " 'embeddings': array([[ 0.03992383, -0.03474854,  0.00143857, ..., -0.04187942,\n",
       "         -0.03481762, -0.00952303],\n",
       "        [ 0.04639997,  0.00105297, -0.03049378, ..., -0.01306829,\n",
       "          0.00158955, -0.01632661],\n",
       "        [ 0.03762385, -0.00716431, -0.02531052, ..., -0.03315016,\n",
       "          0.00358444, -0.02906256],\n",
       "        ...,\n",
       "        [ 0.01713736, -0.02314584, -0.01964445, ..., -0.00968083,\n",
       "          0.02411125, -0.0210966 ],\n",
       "        [ 0.01552705,  0.01941189, -0.03063391, ..., -0.01251121,\n",
       "         -0.01839895, -0.02190708],\n",
       "        [ 0.01649883,  0.02576101, -0.0083029 , ..., -0.0088084 ,\n",
       "          0.00839421,  0.00105229]], shape=(10, 768)),\n",
       " 'documents': ['TABLE OF CONTENTS\\nChapter No.\\nTitle\\nPage No.\\n1.\\nINTRODUCTION\\n01\\n2.\\nPROBLEM DEFINITION\\n02\\n3.\\nLITERATURE SURVEY\\n03-18\\n3.1 Celeb-DF: A Large-Scale Challenging Dataset for\\nDeepFake Forensics\\n03\\n3.1.1 Abstract\\n03\\n3.1.2 Problems addressed\\n03\\n3.1.3 Methodology\\n04\\n3.1.4 Limitations\\n04\\n3.2 Image Feature Detectors for Deepfake Video Detection\\n05\\n3.2.1 Abstract\\n05\\n3.2.2 Problems addressed\\n05\\n3.2.3 Methodology\\n05\\n3.2.4 Advantages\\n05\\n3.2.5 Limitations\\n06\\n3.3 Multi-task Learning For Detecting and Segmenting\\nManipulated Facial Images and Videos\\n07\\n3.3.1 Abstract\\n07\\n3.3.2 Problems addressed\\n07\\n3.3.3 Solutions Provided\\n07\\n3.3.4 Advantages\\n08\\n3.3.5 Disadvantage\\n08\\n3.4 VIOLA jones algorithm with capsule graph network\\nfor Deepfake detection\\n09\\n\\n3.4.1 Abstract\\n09\\n3.4.2 Problems addressed\\n09\\n3.4.3 Methodology\\n10\\n3.4.4 Advantages\\n10\\n3.4.5 Limitations\\n10\\n3.5 Multi-Label Deepfake Classification\\n11\\n3.5.1 Abstract\\n11\\n3.5.2 Problems addressed\\n11\\n3.5.3 Methodology\\n11\\n3.5.4 Advantages\\n12\\n3.5.5 Limitations\\n12\\n3.6 Detection and Localization of Facial Expression Manipulation\\n13\\n3.6.1 Abstract\\n13\\n3.6.2 Problems addressed\\n13\\n3.6.3 Solution Provided\\n13\\n3.6.4 Advantages\\n13\\n3.6.5 Disadvantages\\n14\\n3.7 Implementation of Dlib Deep Learning Face Recognition\\nTechnology\\n15\\n3.7.1 Abstract\\n15\\n3.7.2 Problems addressed\\n15\\n3.7.3 Solution Provided\\n15\\n3.7.4 Advantages\\n15\\n3.7.5 Disadvantages\\n16\\n3.8 Generation of Controlled Face Images using Curated Datasets\\nand StyleGAN2\\n17\\n3.8.1 Abstract\\n17\\n\\n3.8.2 Problems addressed\\n17\\n3.8.3 Methodology\\n18\\n3.8.4 Limitations\\n18\\n4.\\nDATA\\n19\\n4.1 Overview\\n19\\n5.\\nSYSTEM REQUIREMENTS SPECIFICATION\\n20-24\\n5.1 Project Scope\\n20\\n5.2 Project Perspective\\n21\\n5.2.1 Product Features\\n21\\n5.2.2 Operating Environment\\n21\\n5.2.3 General Constraints, Assumptions and\\nDependencies\\n22\\n5.2.4 Risks\\n22\\n5.3 Functional Requirements\\n23\\n5.4 Non-Functional Requirements\\n24\\n5.5 Other Requirements\\n25\\n5.5.1 Data Requirements\\n25\\n6.\\nSYSTEM DESIGN DETAILS\\n26-31\\n6.1 Current system\\n26\\n6.2 Proposed Methodology\\n28\\n6.3 Use Case\\n31\\n6.4 State Diagram\\n31\\n6.5 Design Details\\n32\\n6.5.1 Novelty\\n32\\n6.5.2 Innovativeness\\n32\\n6.5.3 Performance\\n32\\n6.5.4 Security\\n33\\n\\n6.5.5 Reliability\\n33\\n6.5.6 Reusability\\n33\\n7.\\nIMPLEMENTATION AND PSEUDOCODE\\n34\\n7.1 PSUEDOCODE 34\\n7.2 OUTPUT 34\\n8.\\nCONCLUSION OF CAPSTONE PROJECT PHASE ‚Äì 1\\n35\\n9.\\nPLAN OF WORK FOR CAPSTONE PROJECT\\nPHASE ‚Äì 2\\n36\\n10.\\nREFERENCES/BIBLIOGRAPHY\\n37\\n11.\\nAPPENDIX A : DEFINITIONS, ACRONYMS\\nAND ABBREVIATIONS\\n39\\n\\nLIST OF FIGURES\\nFigure No.\\nTitle\\nPage No.\\n6.2.1\\nArchitecture diagram\\n28\\n6.2.2\\nEAR stream\\n29\\n6.2.3\\nEncoder\\n29\\n6.2.4\\nDecoder\\n30\\n6.2.5\\nLocalization of Deepfaked regions and\\ngenerate % for Extent of Deepfake\\n30\\n6.3\\nUse Case\\n31\\n6.4\\nState diagram\\n31\\n7.2\\nContour Detection\\n34',\n",
       "  'LIST OF FIGURES\\nFigure No.\\nTitle\\nPage No.\\n6.2.1\\nArchitecture diagram\\n28\\n6.2.2\\nEAR stream\\n29\\n6.2.3\\nEncoder\\n29\\n6.2.4\\nDecoder\\n30\\n6.2.5\\nLocalization of Deepfaked regions and\\ngenerate % for Extent of Deepfake\\n30\\n6.3\\nUse Case\\n31\\n6.4\\nState diagram\\n31\\n7.2\\nContour Detection\\n34\\n\\nPage No. 1\\nDept. of CSE\\nJan - May, 2024\\nCHAPTER 1\\nINTRODUCTION\\nAs technology advances, the proliferation of deep fake images poses a significant challenge to\\ndiscerning reality from manipulation. While traditional deepfake detection methods primarily rely on\\nbinary classification‚Äîlabeling images as either real or fake‚Äîthey often lack the necessary\\ntransparency and interpretability required for real-world applications.\\nRather than solely focusing on binary predictions, our approach focuses on detecting deep fakes and\\nmainly on localizing and identifying manipulated facial expressions and attributes using EAR stream\\nsystem and encoder-decoder. It aims to localize the manipulated regions of the input images by\\ngenerating a CAM highlighting deep-faked regions.\\nThen to describe the deepfaked region textually, CAM is passed onto the HSV color space for more\\nintuitive color manipulation and analysis. Following contour detection, a bounding box is drawn\\naround the identified contoured region using the Dlib enabling precise localization of features within\\nthe image. Following 68 landmark iterations, a textual description about the alteration of facial\\nfeatures is produced. Our method provides a more thorough knowledge of the underlying\\nmodifications by extracting and analysing these traits, allowing users to distinguish minute\\ndifferences between modified and real photos.\\nMoreover, we acknowledge the significance of measuring the degree of alteration in facial areas. In\\norder to mitigate this, the resulting feature map is subjected to a quantitative assessment of the depth\\nof deep false detection by calculating the percentage of contour detected area.',\n",
       "  'Page No. 2\\nDept. of CSE\\nJan - May, 2024\\nPROBLEM DEFINITION\\nAs technology develops, it becomes more difficult to distinguish between actual and\\nmanipulated photos due to the widespread occurrence of deepfake images. While binary\\nclassification‚Äîclassifying images as either real or fake‚Äîis the mainstay of previous\\ndeepfake detection approaches, they frequently lack the transparency and interpretability\\nneeded for practical use.\\nOur method explores the subtleties of facial manipulation by producing textual descriptions\\nthat emphasise modifications to important features like eyes, noses, and lips as well as\\ncharacteristics like fringe, glasses, beards, and so on, instead of concentrating only on\\nbinary predictions. Our method provides a more thorough knowledge of the underlying\\nmodifications by extracting and analysing these traits, allowing users to distinguish minute\\ndifferences between modified and real photos.\\nFurthermore, we recognize the importance of quantifying the extent of manipulation within\\nfacial regions. To address this, our system calculates the total percentage of the deepfaked\\narea within the facial region, providing users with a quantitative measure of the level of\\nmanipulation present in an image.',\n",
       "  'Page No.3\\nDept. of CSE\\nJan - May, 2024\\n3. LITERATURE SURVEY\\n3.1 Celeb-DF: A Large-scale Challenging Dataset for DeepFake\\nForensics\\nY. Li, X. Yang, P. Sun, H. Qi and S. Lyu,\\nCeleb-DF:A Large-Scale Challenging Dataset for DeepFake\\nForensics\\nhttps://ieeexplore.ieee.org/abstract/document/9156368\\n3.1.1 Abstract\\n‚Ä¢ Presents Celeb-DF, a brand-new, really difficult DeepFake video dataset\\nthat contains 5,639 excellent DeepFake films.\\n‚Ä¢ Improved synthesis techniques were used to create DeepFake videos with fewer\\nvisual artefacts.\\n‚Ä¢ Carries out an extensive analysis of DeepFake detection techniques and\\ndatasets to illustrate the more difficult problems that Celeb-DF presents.\\n3.1.2 Problems Addressed\\n‚Ä¢ Current DeepFake datasets have noticeable artifacts that deviate from\\nauthentic DeepFake videos that are shared online, in addition to having\\npoor visual quality.\\n‚Ä¢ A more difficult and realistic DeepFake video dataset is required in order\\nto assess detection techniques more effectively.\\n‚Ä¢ Insufficient assessment of DeepFake detection techniques on other, superior\\nDeepFake datasets.\\n\\nPage No.4\\nDept. of CSE\\nJan - May, 2024\\n3.1.3 Methodology\\n‚Ä¢ Used 590 real YouTube videos of 59 celebrities as source videos.\\n‚Ä¢ Generated 5,639 DeepFake videos using improved synthesis algorithm\\naddressing artifacts like low resolution, color mismatch, inaccurate face masks,\\ntemporal flickering.\\n‚Ä¢ Evaluated 9 state-of-the-art DeepFake detection methods on Celeb-DF and\\nother datasets using frame-level AUC as the metric.\\n3.1.4 Limitations\\n‚Ä¢\\nWhile visual quality is improved, forgers can adopt anti-forensic\\ntechniques to hide DeepFake traces, which are not incorporated in Celeb-\\nDF.\\n‚Ä¢\\nRunning efficiency and model structure of synthesis algorithm needs\\nimprovement for further enhancing visual quality.\\n‚Ä¢\\nDataset size and diversity could be further increased',\n",
       "  'Page No.5\\nDept. of CSE\\nJan - May, 2024\\n3.2 Image Feature Detectors for Deepfake Video Detection\\nF. F. Kharbat, T. Elamsy, A. Mahmoud and R.\\nAbdullah Image Feature Detectors for Deepfake\\nVideo Detection\\nhttps://ieeexplore.ieee.org/document/9035360\\n3.2.1 Abstract\\n‚óè There are many tools available on the Internet that can produce convincing\\nDeepfake videos with minimal effort. Thus, detecting deepfakes has become\\nmore challenging.\\n‚óè This paper compares and explores the use of algorithms that extracts feature points\\nfrom video frames. Then, The features extracted from Various algorithms like\\nHOG, SURF, KAZE, ORB and so on are passed to the Support Vector Machine in\\norder to detect if the video is deepfake or not.\\n‚óè The results showed that HOG out of all the feature detectors showed the best\\nperformance with 95 % accuracy.\\n3.2.2 Problems Addressed\\n‚óè Difficulty in detection of deepfake in videos because of constant improvement\\nin generating convincing realistic deepfakes.\\n3.2.3 Methodology\\n‚óè Using a feature extractor or detector to extract important features from various\\nvideo frames. These features are passed as input to the Support Vector Machine\\nto classify whether they are deepfake or not\\n3.2.4 Advantages\\n‚óè The extracted frames only focus on important features that can capture\\ninconsistencies like differences in lighting conditions and so on, giving a better\\nperformance in terms of accuracy.\\n‚óè Using this also reduces the size of images and hence, fastens the processing time.\\n\\nPage No.6\\nDept. of CSE\\nJan - May, 2024\\n3.2.5 Limitations\\n‚óè This proposed model might struggle with unseen deepfake variations because it\\nhas used only 98 videos for training.\\n‚óè It also doesn‚Äôt consider temporal dependencies between the frames,\\nwhich could have significantly improved the performance',\n",
       "  'Page No.6\\nDept. of CSE\\nJan - May, 2024\\n3.2.5 Limitations\\n‚óè This proposed model might struggle with unseen deepfake variations because it\\nhas used only 98 videos for training.\\n‚óè It also doesn‚Äôt consider temporal dependencies between the frames,\\nwhich could have significantly improved the performance\\n\\nPage No.7\\nDept. of CSE\\nJan - May, 2024\\n3.3 Multi-task Learning For Detecting and Segmenting\\nManipulated Facial Images and Videos\\nHuy H. Nguyen, Fuming Fang, Junichi Yamagishi, Isao Echizen\\nMulti-task Learning For Detecting and Segmenting Manipulated Facial Images and Videos\\nhttps://arxiv.org/abs/1906.06876\\n3.3.1 Abstract\\nImages and vidoes can be easily manipulated by using some of the common attacks like\\nremoval, splicing and copy-move. Thus , detecting the manipulation and locating the\\nmanipulated region if the image is manipulated is the important concern in this paper. This\\npaper proposes an architecture that does both by implementing an encoder-decoder with\\nthe decoder being Y- shaped. While the encoder provides the encoded representation of the\\ninput whose activation can be used for classification, one branch of the decoder does\\nreconstruction whereas the other branch provides segmentation map containing the\\nmanipulated regions as output. Results show that this paper achieved 93% accuracy in\\nterms of segmentation.\\n3.3.2 Problem Addressed\\nThe first concern was classifying if the facial images were manipulated or not. The\\nsecond concern if the image was classified as manipulated, it was necessary to detect\\nwhat parts of the images that were manipulated.\\n3.3.3 Solution provided\\nA multi-task learning approach is used in which encoder with Y-shaped decoder is trained\\nfor detecting and segmenting manipulated region',\n",
       "  \"Page No.8\\nDept. of CSE\\nJan - May, 2024\\n3.3.4 Advantages\\n‚óè While usual methods focusses on binary classification, this approach does both\\nclassification as well as segmentation.\\n‚óè Y-shape in the decoder allows autoencoder to share information between\\nclassification branch and reconstruction branch and hence, improve overall\\nperformance.\\n3.3.5 Disadvantages\\n‚óè Computational Complexity and training time maybe high.\\n‚óè The paper worked only on normalised images and not on residual images. There\\nis a need for investigating the effect of using residual images on the autoencoder‚Äôs\\nperformance, processing high-resolution images without re-sizing, improving its\\nability to deal with unseen attacks\\n\\nPage No.9\\nDept. of CSE\\nJan - May, 2024\\n3.4 VIOLA jones algorithm with capsule graph network for\\nDeepfake detection\\nK V, Trojovsk√Ω P, Hub√°lovsk√Ω ≈†.\\nVIOLA jones algorithm with capsule graph network for\\nDeepfake detection https://doi.org/10.7717/peerj-cs.1313\\n3.4.1 Abstract\\nThe research paper proposes a novel approach for detecting fake images and videos, known\\nas DeepFake, using deep learning techniques. The paper introduces the VIOLA Jones\\nalgorithm with the Capsule Graph Neural Network (CN) to enhance accuracy of detecting\\nfake images. The research evaluates the proposed model using the CelebDF-\\nFaceForencics++ (c23) datasets and achieves an impressive accuracy of 94%.\\n3.4.2 Problems Addressed\\nBy learning the graph in a neural network and addressing nodes,CGNN makes sure the\\nembedding problem. Using the CGNN and VIOLA Jones's advantages, this study makes the\\nfollowing contributions:\\n‚óè 1.By employing graph and tracing models for deepfake detection, we are\\nincreasing detection accuracy.\\n‚óè 2. The suggested work's computation time is shortened by the quick detection and\\ntraining of node features.\\n‚óè 3. The routing mechanism's of CGNN utilizes the node features to increase\\nthe detection's accuracy.\\n\\nPage No.10\\nDept. of CSE\\nJan - May, 2024\\n3.4.3 Methodology\\nThe VIOLA-Jones algorithm used in this research is used to detect faces/facial features\\nfrom the input images . The input faked image is divided into sub-windows(smaller sizes).\\nIt identifies all facial features in the sub-window .Then resultant predicted faces is analyzed\\nusing a CGNN .\\n3.4.4 Advantages\\n‚óè This model needs less computation time than the existing models. Input data validation\\nis generated to compare loss and accuracy of the deepfaked images dataset for real-time\\nand large batches of data.\\n‚óè The prediction error is less than in existing models across various datasets\\n3.4.5 Limitations\\n‚óè Although, the system has less error rate,swarm-based optimization techniques can be\\nused to optimize the feature selection and enhance accuracy and reduce prediction error\\nof the current model.\",\n",
       "  'Page No.11\\nDept. of CSE\\nJan - May, 2024\\n3.5 Multi-label Deepfake Classification\\nSingh, I. P., Mejri, N., Nguyen, V. D., Ghorbel, E.,\\n& Aouada, D. Multi-Label Deepfake Classification.\\nhttps://orbilu.uni.lu/handle/10993/55909\\n3.5.1 Abstract\\nThe paper researches deepfake detection towards multi- label classification, with reference\\nto stacked manipulations in images. It evaluates both direct (ResNet, TResNet) and indirect\\n(ML-GCN, IML- GCN, ML-AGCN) approaches.\\n3.5.2 Problems Addressed\\nThe research addresses the problem of binary classification in deepfake detection, which\\nlacks interpretability and explainability. It argues that deepfake images often result from\\nmulti-step manipulations with various properties, hence making it necessary to recognize\\nthe reason for these stacked manipulations. The authors propose to reframe the method of\\ndeepfake detection as a multi-label image classification problem, aiming to predict the\\npresence of multiple manipulations simultaneously.\\n3.5.3 Methodology\\nThe paper presents an overview of multi-label image classification techniques, categorizing\\nthem into direct(ResNet, TResNet) and indirect methods(ML-GCN, IML-GCN, ML-\\nAGCN). Direct methods employ deep neural networks for feature extraction and\\nclassification, while indirect methods model label correlations using graph-based\\napproaches. The authors assess the performance of these methods in the context of deepfake\\ndetection through extensive experiments using a deepfake dataset with multi- label\\nannotations.\\n\\nPage No.12\\nDept. of CSE\\nJan - May, 2024\\n3.5.4 Advantages\\n‚óè By identifying multiple categories of image manipulations, the multi-label\\napproach allows for more interpretable predictions than binary predictions, which\\nare opaque and challenging to understand.\\n‚óè It draws a comparison between effectiveness of direct and indirect methods in\\ndetecting stacked manipulations,they observe that direct methods, particularly\\nResNet, outperform indirect methods in the context of multi-label deepfake\\nclassification\\n3.5.5 Limitations\\n‚óè While the Deep-Seq dataset used in the paper incorporates multi-label\\nannotations, it may not fully capture the complexity and diversity of real-world\\ndeepfake scenarios.\\n‚óè We can observe that compared to the facial attribute subset, the facial components\\nmanipulation subset is comparatively harder, particularly for indirect methods that\\ninclude a large standard deviation.',\n",
       "  \"Page No.13\\nDept. of CSE\\nJan - May, 2024\\n3.6 Detection and Localization of Facial Expression Manipulations\\nG. Mazaheri and A. K. Roy-Chowdhury,\\nDetection and Localization of Facial Expression Manipulations\\nhttps://ieeexplore.ieee.org/document/9706668\\n3.6.1 Abstract\\nThe widespread usage of faked photos and videos on social media raises concerns about\\nthe need for accurate identification of this type of fraud. Expression swapping and Identity\\nswapping (DeepFake) can be used to generate facial modifications. Expression swap\\ndetection has not received as much attention as identity swap detection, which is easily\\nidentified using new deepfake detection techniques. It is well recognized that facial\\nexpressions play a crucial role in interpersonal communication.\\nTherefore, it's critical to create techniques that can identify and pinpoint facial expression\\nmodifications.\\n3.6.2 Problem Addressed\\nThe challenge is maintaining high performance in identifying manipulated facial expressions\\nwithout compromising accuracy.\\n3.6.3 Solution provided\\nEncoder- Decoder architecture known as deeplabv3+ is implemented for manipulation detection\\nand segmentation as the task of localizing manipulation regions\\n3.6.4 Advantages\\n‚óè Accurately identifies manipulated features, surpassing existing methods. Superior\\nperformance on datasets with abundant expression manipulations.\\n‚óè Successful in targeting specific facial regions for manipulation. Robust against\\nidentity swaps. Therefore, offering a comprehensive solution for facial\\nmanipulation detection.\\n\\nPage No.14\\nDept. of CSE\\nJan - May, 2024\\n3.6.5 Disadvantages\\n‚óè There may be significant computational complexity and training duration .\\n‚óè Robustness to variations in lighting\\n‚óè Risques arise from relying too much on precise recognition models and\\nsmall evaluation datasets. Furthermore, modifications must be made\\nfrequently to accommodate changing manipulation methods.\",\n",
       "  \"Page No.14\\nDept. of CSE\\nJan - May, 2024\\n3.6.5 Disadvantages\\n‚óè There may be significant computational complexity and training duration .\\n‚óè Robustness to variations in lighting\\n‚óè Risques arise from relying too much on precise recognition models and\\nsmall evaluation datasets. Furthermore, modifications must be made\\nfrequently to accommodate changing manipulation methods.\\n\\nPage No.15\\nDept. of CSE\\nJan - May, 2024\\n3.7 Implementation of Dlib Deep Learning Face Recognition\\nTechnology\\nHuy H. Nguyen, Fuming Fang, Junichi Yamagishi, Isao\\nEchizen Implementation of Dlib Deep Learning Face\\nRecognition Technology https://arxiv.org/abs/1906.06876\\n3.7.1 Abstract\\nA new face recognition technique using Dlib and the ERT algorithm to address issues like\\nmissing or false detections and poor recognition. This Python-based approach is great\\nbecause it's resistant to occlusions, like when parts of the face are covered.This method\\nleads to better recognition, increased sensitivity, and more precise detection.\\n3.7.2 Problem Addressed\\nFace recognition has become a major focus in artificial intelligence lately, helping save a\\nton of manual work. However, the OpenCV algorithm used for this has some issues. It\\nsometimes gets faces wrong and doesn't always recognize them accurately.\\n3.7.3 Solution provided\\nThis paper talks about a deep learning method for recognizing faces using convolutional\\nneural networks. Basically, it's a process where the system first finds the face, then figures\\nout where the important features are on that face, and finally matches it up with known\\nfaces using a calculation called Euclidean distance.\\n3.7.4 Advantages\\n‚óè Its benefit is the usage of an open-source Python interface library, which can\\nprovide improved accuracy for face detection and efficiency is high for static\\npicture, dynamic video, and real- time camera video recognition.\"],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'embeddings'],\n",
       " 'data': None,\n",
       " 'metadatas': [{'section': '3.1',\n",
       "   'total_chunks': 22,\n",
       "   'source': '7_Report - khushi kiran.pdf',\n",
       "   'word_count': 410,\n",
       "   'chunk_id': 0,\n",
       "   'position_ratio': 0.0,\n",
       "   'chunk_length': 2692,\n",
       "   'potential_title': 'TABLE OF CONTENTS',\n",
       "   'preview': 'TABLE OF CONTENTS Chapter No. Title Page No. 1. INTRODUCTION 01 2. PROBLEM DEFINITION 02 3. LITERATURE SURVEY 03-18 3.1 Celeb-DF: A Large-Scale Challenging Dataset for DeepFake Forensics 03 3.1.1 Abst'},\n",
       "  {'position_ratio': 0.045454545454545456,\n",
       "   'chunk_length': 1940,\n",
       "   'source': '7_Report - khushi kiran.pdf',\n",
       "   'section': '2.1',\n",
       "   'preview': 'LIST OF FIGURES Figure No. Title Page No. 6.2.1 Architecture diagram 28 6.2.2 EAR stream 29 6.2.3 Encoder 29 6.2.4 Decoder 30 6.2.5 Localization of Deepfaked regions and generate % for Extent of Deepf',\n",
       "   'chapter': '1',\n",
       "   'potential_title': 'LIST OF FIGURES',\n",
       "   'total_chunks': 22,\n",
       "   'chunk_id': 1,\n",
       "   'word_count': 287},\n",
       "  {'chunk_length': 1230,\n",
       "   'chunk_id': 2,\n",
       "   'source': '7_Report - khushi kiran.pdf',\n",
       "   'position_ratio': 0.09090909090909093,\n",
       "   'word_count': 176,\n",
       "   'preview': 'Page No. 2 Dept. of CSE Jan - May, 2024 PROBLEM DEFINITION As technology develops, it becomes more difficult to distinguish between actual and manipulated photos due to the widespread occurrence of de',\n",
       "   'total_chunks': 22},\n",
       "  {'total_chunks': 22,\n",
       "   'source': '7_Report - khushi kiran.pdf',\n",
       "   'position_ratio': 0.13636363636363635,\n",
       "   'chunk_length': 1846,\n",
       "   'section': '3.1',\n",
       "   'chunk_id': 3,\n",
       "   'word_count': 262,\n",
       "   'preview': 'Page No.3 Dept. of CSE Jan - May, 2024 3. LITERATURE SURVEY 3.1 Celeb-DF: A Large-scale Challenging Dataset for DeepFake Forensics Y. Li, X. Yang, P. Sun, H. Qi and S. Lyu, Celeb-DF:A Large-Scale Chal'},\n",
       "  {'source': '7_Report - khushi kiran.pdf',\n",
       "   'section': '3.2',\n",
       "   'preview': 'Page No.5 Dept. of CSE Jan - May, 2024 3.2 Image Feature Detectors for Deepfake Video Detection F. F. Kharbat, T. Elamsy, A. Mahmoud and R. Abdullah Image Feature Detectors for Deepfake Video Detectio',\n",
       "   'word_count': 281,\n",
       "   'chunk_length': 1797,\n",
       "   'total_chunks': 22,\n",
       "   'chunk_id': 4,\n",
       "   'position_ratio': 0.18181818181818185},\n",
       "  {'chunk_id': 5,\n",
       "   'source': '7_Report - khushi kiran.pdf',\n",
       "   'total_chunks': 22,\n",
       "   'chunk_length': 1738,\n",
       "   'preview': 'Page No.6 Dept. of CSE Jan - May, 2024 3.2.5 Limitations ‚óè This proposed model might struggle with unseen deepfake variations because it has used only 98 videos for training. ‚óè It also doesn‚Äôt conside',\n",
       "   'word_count': 261,\n",
       "   'section': '2.5',\n",
       "   'position_ratio': 0.2272727272727273},\n",
       "  {'chunk_id': 6,\n",
       "   'chunk_length': 2769,\n",
       "   'position_ratio': 0.2727272727272727,\n",
       "   'word_count': 416,\n",
       "   'total_chunks': 22,\n",
       "   'source': '7_Report - khushi kiran.pdf',\n",
       "   'preview': 'Page No.8 Dept. of CSE Jan - May, 2024 3.3.4 Advantages ‚óè While usual methods focusses on binary classification, this approach does both classification as well as segmentation. ‚óè Y-shape in the decode',\n",
       "   'section': '3.4'},\n",
       "  {'position_ratio': 0.3181818181818182,\n",
       "   'total_chunks': 22,\n",
       "   'source': '7_Report - khushi kiran.pdf',\n",
       "   'chunk_length': 2421,\n",
       "   'word_count': 323,\n",
       "   'preview': 'Page No.11 Dept. of CSE Jan - May, 2024 3.5 Multi-label Deepfake Classification Singh, I. P., Mejri, N., Nguyen, V. D., Ghorbel, E., & Aouada, D. Multi-Label Deepfake Classification. https://orbilu.un',\n",
       "   'section': '3.5',\n",
       "   'chunk_id': 7},\n",
       "  {'chunk_length': 1917,\n",
       "   'source': '7_Report - khushi kiran.pdf',\n",
       "   'total_chunks': 22,\n",
       "   'word_count': 254,\n",
       "   'section': '3.6',\n",
       "   'chunk_id': 8,\n",
       "   'position_ratio': 0.3636363636363637,\n",
       "   'preview': 'Page No.13 Dept. of CSE Jan - May, 2024 3.6 Detection and Localization of Facial Expression Manipulations G. Mazaheri and A. K. Roy-Chowdhury, Detection and Localization of Facial Expression Manipulat'},\n",
       "  {'word_count': 276,\n",
       "   'chunk_id': 9,\n",
       "   'preview': 'Page No.14 Dept. of CSE Jan - May, 2024 3.6.5 Disadvantages ‚óè There may be significant computational complexity and training duration . ‚óè Robustness to variations in lighting ‚óè Risques arise from rely',\n",
       "   'position_ratio': 0.4090909090909091,\n",
       "   'chunk_length': 1861,\n",
       "   'source': '7_Report - khushi kiran.pdf',\n",
       "   'section': '6.5',\n",
       "   'total_chunks': 22}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.peek()  # Or use collection.get() to see chunk IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "862e95f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ollama integration ready!\n",
      "Usage: test_qa() or chat_with_rag()\n"
     ]
    }
   ],
   "source": [
    "# Cell 19: Ollama LLM Integration\n",
    "import ollama\n",
    "\n",
    "def answer_with_ollama(question, n_chunks=3, model=\"llama3.1:8b-instruct-q4_0\", temperature=0.3):\n",
    "    \"\"\"\n",
    "    Answer questions using local Llama 3.1 with RAG context\n",
    "    \"\"\"\n",
    "    print(f\"ü§î Processing question: {question}\")\n",
    "    \n",
    "    # Get context from RAG\n",
    "    context = build_context_for_chatbot(question, n_chunks=n_chunks, min_relevance=0.3)\n",
    "    \n",
    "    # Check if we found relevant context\n",
    "    if \"couldn't find sufficiently relevant\" in context:\n",
    "        return \"I don't have relevant information in the documents to answer this question. Please try rephrasing or ask about topics covered in the uploaded documents.\"\n",
    "    \n",
    "    # Simple prompt\n",
    "    prompt = f\"\"\"Based on the following context from research documents, please answer the question accurately.\n",
    "If the context doesn't contain enough information, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"üöÄ Running {model}...\")\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {'role': 'user', 'content': prompt}\n",
    "            ],\n",
    "            options={\n",
    "                'temperature': temperature,\n",
    "                'num_predict': 500\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return response['message']['content']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        print(\"Make sure Ollama is running: 'ollama serve'\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def chat_with_rag():\n",
    "    \"\"\"\n",
    "    Simple interactive chat\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ RAG Chatbot with Llama 3.1\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    # Check Ollama\n",
    "    try:\n",
    "        ollama.list()\n",
    "        print(\"‚úÖ Ollama is running!\\n\")\n",
    "    except:\n",
    "        print(\"‚ùå Start Ollama first: run 'ollama serve' in terminal\")\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\n‚ùì Your question: \").strip()\n",
    "        \n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not question:\n",
    "            continue\n",
    "        \n",
    "        answer = answer_with_ollama(question)\n",
    "        print(\"\\nüí° Answer:\")\n",
    "        print(answer)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Quick test\n",
    "def test_qa():\n",
    "    \"\"\"Test with a simple question\"\"\"\n",
    "    question = \"What methodologies are discussed for plant disease detection?\"\n",
    "    print(f\"‚ùì Test Question: {question}\\n\")\n",
    "    \n",
    "    answer = answer_with_ollama(question)\n",
    "    print(f\"üí° Answer: {answer}\")\n",
    "\n",
    "print(\"‚úÖ Ollama integration ready!\")\n",
    "print(\"Usage: test_qa() or chat_with_rag()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48a34417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§î Processing question: What is discussed about plant disease detection?\n",
      "üîç Searching for: 'What is discussed about plant disease detection?'\n",
      "üéØ Reranking 12 candidates...\n",
      "üìã Found 12 candidates, 10 above threshold, returning top 3:\n",
      "\n",
      "1. üìÑ bs.pdf (ID: chunk_14)\n",
      "   üìñ Chapter 6\n",
      "   üìë Section 6.1\n",
      "   üìä Initial: 0.746, Rerank: 5.545, Final: 4.105\n",
      "   üìè Chunk size: 1723 chars\n",
      "   üìù Preview: 28 of 46\n",
      "CHAPTER 6\n",
      "System Design\n",
      "6.1 Current System\n",
      "The systems that exist for plant disease detection mainly follow a two-step process which is to identify\n",
      "if the plant is healthy or unhealthy using ...\n",
      "\n",
      "2. üìÑ bs.pdf (ID: _chunk_2)\n",
      "   üìñ Chapter 2\n",
      "   üìë Section 3.1\n",
      "   üìä Initial: 0.746, Rerank: 5.426, Final: 4.022\n",
      "   üìè Chunk size: 2603 chars\n",
      "   üìù Preview: 12 of 46\n",
      "CHAPTER 2\n",
      "Problem Definition\n",
      "The existing manual methods for plant disease detection in agriculture are inefficient and prone to\n",
      "errors, leading to significant loss of crops and are a huge co...\n",
      "\n",
      "3. üìÑ bs.pdf (ID: chunk_10)\n",
      "   üìñ Chapter 5\n",
      "   üìë Section 5.1\n",
      "   üìä Initial: 0.742, Rerank: 3.926, Final: 2.971\n",
      "   üìè Chunk size: 1727 chars\n",
      "   üìù Preview: 23 of 46\n",
      "CHAPTER 5\n",
      "Systems Requirements Specification\n",
      "5.1 Product Perspective\n",
      "5.1.1 Product Features\n",
      "1) Automated Disease Detection: Our plant disease detection model automatically detects plant\n",
      "disea...\n",
      "üöÄ Running llama3.1:8b-instruct-q4_0...\n",
      "Based on the provided context, here's what is discussed about plant disease detection:\n",
      "\n",
      "* The current systems for plant disease detection follow a two-step process of identifying healthy or unhealthy plants using images and then classifying specific diseases (Source 1).\n",
      "* The problem with current classification models is that they use legacy datasets and struggle to classify new diseases, so the intention is to use Clustering Algorithms and GANs to increase the dataset (Source 1).\n",
      "* The design goals include Enhanced Disease Detection Accuracy, Improved User Experience, and Advanced Technologies Used (Source 1).\n",
      "* The existing manual methods for plant disease detection are inefficient and prone to errors, leading to significant loss of crops and concerns about food reliability and security (Source 2).\n",
      "* A system is proposed that can accurately identify plant leaf diseases from leaf images using deep learning frameworks, image processing techniques, and GANs (Source 2).\n",
      "* Clustering algorithms and industry experts are used as a novel approach to classify unrecognized images (Source 2).\n",
      "* The goal is to provide farmers with a reliable tool for detecting plant diseases, which will help them in farming efficiently and lead to better crop production and food security (Source 2).\n",
      "* A paper proposes a Leafgan model that addresses overfitting in plant disease detection models by generating synthetic images of diseased plants from healthy ones using GANs, attention mechanisms, and CycleGAN (Source 3).\n",
      "* The limitations of the Leafgan model include domain specificity, background complexity, and limited testing on generalization (Source 3).\n",
      "\n",
      "Overall, the context discusses various aspects of plant disease detection, including current systems, design goals, proposed solutions, and research papers exploring new approaches to improve accuracy and efficiency in detecting plant diseases.\n"
     ]
    }
   ],
   "source": [
    "# Run this in a new cell to test\n",
    "answer = answer_with_ollama(\"What is discussed about plant disease detection?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d8f62b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Total documents in collection: 122\n"
     ]
    }
   ],
   "source": [
    "print(f\"üì¶ Total documents in collection: {collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a82d78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"üìÅ Exists:\", os.path.exists(\"chromadb_store\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f721e0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Collection name: rag-chunks\n",
      "üì¶ Total documents: 122\n",
      "üß™ Client type: <class 'chromadb.api.client.Client'>\n",
      "üìç DB location (guess): C:\\Users\\jsdha\\AppData\\Roaming\\Python\\Python313\\site-packages\\chromadb\\api\\client.py\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(\"üìÇ Collection name:\", collection.name)\n",
    "print(\"üì¶ Total documents:\", collection.count())\n",
    "\n",
    "# Inspect client object to confirm it's a PersistentClient\n",
    "print(\"üß™ Client type:\", type(chroma_client))\n",
    "\n",
    "# Optional: Look into where it's storing (if PersistentClient)\n",
    "print(\"üìç DB location (guess):\", inspect.getsourcefile(type(chroma_client)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d5c94cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§î Processing question: What is the methodology for plant disease detection?\n",
      "üîç Searching for: 'What is the methodology for plant disease detection?'\n",
      "üéØ Reranking 12 candidates...\n",
      "üìã Found 12 candidates, 11 above threshold, returning top 3:\n",
      "\n",
      "1. üìÑ bs.pdf (ID: _chunk_2)\n",
      "   üìñ Chapter 2\n",
      "   üìë Section 3.1\n",
      "   üìä Initial: 0.786, Rerank: 7.044, Final: 5.166\n",
      "   üìè Chunk size: 2603 chars\n",
      "   üìù Preview: 12 of 46\n",
      "CHAPTER 2\n",
      "Problem Definition\n",
      "The existing manual methods for plant disease detection in agriculture are inefficient and prone to\n",
      "errors, leading to significant loss of crops and are a huge co...\n",
      "\n",
      "2. üìÑ bs.pdf (ID: chunk_14)\n",
      "   üìñ Chapter 6\n",
      "   üìë Section 6.1\n",
      "   üìä Initial: 0.766, Rerank: 6.551, Final: 4.815\n",
      "   üìè Chunk size: 1723 chars\n",
      "   üìù Preview: 28 of 46\n",
      "CHAPTER 6\n",
      "System Design\n",
      "6.1 Current System\n",
      "The systems that exist for plant disease detection mainly follow a two-step process which is to identify\n",
      "if the plant is healthy or unhealthy using ...\n",
      "\n",
      "3. üìÑ bs.pdf (ID: chunk_10)\n",
      "   üìñ Chapter 5\n",
      "   üìë Section 5.1\n",
      "   üìä Initial: 0.758, Rerank: 5.228, Final: 3.887\n",
      "   üìè Chunk size: 1727 chars\n",
      "   üìù Preview: 23 of 46\n",
      "CHAPTER 5\n",
      "Systems Requirements Specification\n",
      "5.1 Product Perspective\n",
      "5.1.1 Product Features\n",
      "1) Automated Disease Detection: Our plant disease detection model automatically detects plant\n",
      "disea...\n",
      "üöÄ Running llama3.1:8b-instruct-q4_0...\n",
      "\n",
      "üí° Answer:\n",
      "Based on the provided context, the methodologies for plant disease detection mentioned are:\n",
      "\n",
      "1. Manual methods (mentioned as inefficient and prone to errors)\n",
      "2. Deep learning frameworks with image processing techniques and Generative Adversarial Networks (GANs) to classify plant diseases\n",
      "3. Classification Machine Learning Models such as Convolutional Neural Networks (CNNs) to identify healthy or unhealthy plants, followed by further classification into specific disease types if the plant is found unhealthy.\n",
      "4. Clustering Algorithms to group unclassified images and GANs to increase the dataset.\n",
      "\n",
      "Additionally, Source 1 mentions a novel approach using clustering algorithms and industry experts when the model cannot recognize the plant diseases uploaded by the user.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Run test QnA\n",
    "question = \"What is the methodology for plant disease detection?\"\n",
    "answer = answer_with_ollama(question)\n",
    "print(\"\\nüí° Answer:\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
